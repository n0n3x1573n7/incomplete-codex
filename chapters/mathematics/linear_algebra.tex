\documentclass{report}

\begin{document}
The target of Linear Algebra is to solve a system of homogenous linear equations. To do so, we deal with vectors and matrices.
	\section{Vector Spaces}
		For the definitions on vector spaces, subspaces, and bases, refer to the chapter \ref{chap_vector_space}.
		\subsection{Linear Independence}
		We now define linear independence, one of the most important concepts utilized in linear algebra.
		\begin{defn}[Linear Independence]
			if $S=\{\bm{v_1}, \bm{v_2}, \dots, \bm{v_r}\}$ is a nonempty set of vectors in a vector space $V$, then the vector equation $k_1\bm{v_1}+k_2\bm{v_2}+\dots+k_r\bm{v_r}=\bm{0}$ has at least one solution, namely, $k_1=0, k_2=0, \dots, k_r=0$, the \emph{trivial solution}. If this is the only solution, then $S$ is said to be a \emph{linearly independent set}. If there are solutions in addition to the trivial solution, then $S$ is said to be \emph{linearly dependent}.
		\end{defn}
		
		\begin{thm}
			Let $S=\{\bm{v_1}, \bm{v_2}, \dots, \bm{v_r}\}$ be a set of vectors in $\mathbb{R}^n$. If $r>n$, then $S$ is linearly dependent.
		\end{thm}
		
		\subsection{Orthogonality}
		Refer to Chapter \ref{chap_inner_product_space} on information on general inner product spaces and definition on the real inner product space.
		\begin{defn}[Euclidean Inner Product]
			Let $\bm{u}=(u_1,u_2,\dots,u_n)$ and $\bm{v}=(v_1,v_2,\dots,v_n)$ in $\mathbb{R}^n$. The inner product of the two vectors $\bm{u}$ and $\bm{v}$ is defined as
			\begin{displaymath}
				\bm{u}\cdot\bm{v}=\sum_{i=1}^{n}u_iv_i=u_1v_1+u_2v_2+\dots+u_nv_n
			\end{displaymath}
			is called the \emph{Euclidean inner product} or \emph{standard inner product}.
		\end{defn}
		
		We call $R^n$ with the Euclidean inner product \emph{Euclidean n-space}.
		
		\begin{defn}[Euclidean Norm]
			The \emph{norm} of $\bm{u}=(u_1,u_2,\dots,u_n)$ in $\mathbb{R}^n$, denoted $\|\bm{u}\|$, is defined by
			\begin{displaymath}
			\|\bm{u}\|=\sqrt{\bm{u} \cdot \bm{u}}=\sqrt{\sum_{i=1}^{n}u_i^2}=\sqrt{u_1^2+u_2^2+\dots+u_n^2}
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Euclidean Distance]
			If $\bm{u}=(u_1,u_2,\dots,u_n)$ and $\bm{v}=(v_1,v_2,\dots,v_n)$ are vectors in $\mathbb{R}^n$, then the \emph{distance} between $\bm{u}$ and $\bm{v}$, denoted $d(\bm{u},\bm{v})$, and define it to be:
			\begin{displaymath}
				d(\bm{u},\bm{v})=\|\bm{u}-\bm{v}\|=\sqrt{(u_1-v_1)^2+(u_2-v_2)^2+\cdots+(u_n-v_n)^2}
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Unit vectors]
			A vector $\bm{u}$ in $\mathbb{R}^n$ is said to be a unit vector iff $\|\bm{u}\|=1$.
		\end{defn}
		
		\begin{defn}[Angle]
			The \emph{angle} between two nonzero vectors $\bm{u}$ and $\bm{v}$ in $\mathbb{R}^n$ is defined by
			\begin{displaymath}
				\theta=\cos^{-1}(\frac{\bm{u}\cdot\bm{v}}{\|\bm{u}\|\|\bm{v}\|})
			\end{displaymath}
		\end{defn}
		
		\begin{thm}[Cauchy-Schwarz Inequality]
			If $\bm{u}=(u_1,u_2,\dots,u_n)$ and $\bm{v}=(v_1,v_2,\dots,v_n)$ are vectors in $\mathbb{R}^n$, then $\left|\bm{u}\cdot\bm{v}\right| \le \|u\|\|v\|$. In terms of components:
			\begin{displaymath}
				\left|u_1v_1+u_2v_2+\cdots+u_nv_n\right| \le \left(u_1^2+u_2^2+\cdots+u_n^2\right)^{1/2} \left(v_1^2+v_2^2+\cdots+v_n^2\right)^{1/2}
			\end{displaymath}
		\end{thm}
		
		\begin{thm}[Triangle Inequality]
			If $\bm{u}, \bm{v}, \bm{w}$ are vectors in $\mathbb{R}^n$, then:
			\begin{itemize}
				\item $\|\bm{u}+\bm{v}\| \ge \|\bm{u}\|+\|\bm{v}\|$: Triangle Inequality for Vectors
				\item $d(\bm{u},\bm{v}) \ge d(\bm{u},\bm{v})+d(\bm{w},\bm{v})$: Triangle Inequality for Distances
			\end{itemize}
		\end{thm}
		
		\begin{thm}[Equations for Vectors within the Euclidean Space]
			If $\bm{u}$ and $\bm{v}$ are vectors in $\mathbb{R}^n$
			\begin{itemize}
				\item $\|\bm{u}+\bm{v}\|^2+\|\bm{u}-\bm{v}\|^2=2\left(\|u\|^2+\|v\|^2\right)$: Parallelogram Equation for Vectors
				\item $\bm{u}\cdot\bm{v}=\frac{1}{4}\|\bm{u}+\bm{v}\|^2-\frac{1}{4}\|\bm{u}-\bm{v}\|^2$
			\end{itemize}
		\end{thm}
		
		\begin{defn}[Orthogonal Vectors]
			Two nonzero vectors $\bm{u}$ and $\bm{v}$ in $\mathbb{R}^n$ are said to be \emph{orthogonal} or \emph{perpendicular} if $\bm{u} \cdot \bm{v} = 0$.
		\end{defn}
		
		\begin{defn}[Orthogonal set]
			A nonempty set of vectors in $\mathbb{R}^n$ is called an \emph{orthogonal set} if all pairs of distinct vectors in the set are orthogonal. If they are also all unit vectors, it is called an \emph{orthonormal set}.\\
			In other words, for a set $\{u_1, u_2, \dots, u_n\}$ to be orthogonal:
			\begin{displaymath}
				u_i \cdot u_j
				\begin{cases}
					\|u_i\|^2 & i=j\\
					0 & i \ne j
				\end{cases}
			\end{displaymath}
			And for the set to be orthonormal, in addition to above, $\forall i \in \{1, 2, \dots n\}, \|u_i\|=1$.
		\end{defn}
		
		\begin{defn}
			An orthogonal set of nonzero vectors is linearly independent.
		\end{defn}
		
		\begin{defn}[Orthogonal Complement]
			The \emph{orthogonal complement} of a subspace $W$ of an inner product space $V$, denoted $W^\perp$, is defined to be the set of all vectors in $V$ that are orthogonal to every vector of $W$.
		\end{defn}
		
		\begin{thm}
			Suppose $W$ is a subspace of an inner product space $V$.
			\begin{itemize}
				\item $W^\perp$ is a subspace of $V$.
				\item $W \cap W^\perp = \{\bm{0}\}$
			\end{itemize}
		\end{thm}
		
		\begin{thm}
			Suppose $W$ is a subspace of an inner product space $V$. Then, $(W^\perp)^\perp=W$.
		\end{thm}
		
		\begin{thm}\label{thm_projection_formula}
			Let $S=\{\bm{v_1},\bm{v_2},\dots,\bm{v_n}\}$ be a basis for an inner product space $V$.
			\begin{itemize}
					\item If $S$ is an orthogonal basis for $V$, and $\bm{u} \in V$, then
					\begin{displaymath}
						\bm{u}=\frac{\left< \bm{u}, \bm{v_1} \right>}{\|v_1\|^2}\bm{v_1}+
						       \frac{\left< \bm{u}, \bm{v_2} \right>}{\|v_2\|^2}\bm{v_2}+\cdots+
						       \frac{\left< \bm{u}, \bm{v_n} \right>}{\|v_n\|^2}\bm{v_n}
					\end{displaymath}
					And thus
					\begin{displaymath}
						(\bm{u})_S=\left(\frac{\left< \bm{u}, \bm{v_1} \right>}{\|v_1\|^2},
						\frac{\left< \bm{u}, \bm{v_2} \right>}{\|v_2\|^2},\cdots,
						\frac{\left< \bm{u}, \bm{v_n} \right>}{\|v_n\|^2}\right)
					\end{displaymath}
					\item If $S$ is an orthonormal basis for $V$, and $\bm{u} \in V$, then
					\begin{displaymath}
						\bm{u}=\left< \bm{u}, \bm{v_1} \right>\bm{v_1}+
						       \left< \bm{u}, \bm{v_2} \right>\bm{v_2}+\cdots+
						       \left< \bm{u}, \bm{v_n} \right>\bm{v_n}
					\end{displaymath}
					And thus
					\begin{displaymath}
						(\bm{u})_S=\left(\left< \bm{u}, \bm{v_1} \right>,
						\left< \bm{u}, \bm{v_2} \right>,\cdots,
						\left< \bm{u}, \bm{v_n} \right>\right)
					\end{displaymath}
			\end{itemize}
		\end{thm}

		
		\begin{thm}[Projection Theorem]
			If $W$ is a finite-dimensional subspace of an inner product space $V$, then every $\bm{u} \in V$ can be expressed in exactly one way in the form $\bm{u}=\bm{w_1}+\bm{w_2}$, where $\bm{w_1} \in W$ and $\bm{w_2} \in W^\perp$.
		\end{thm}
		
		In the theorem above, the vector $\bm{w_1}$ is called the \emph{orthogonal projection of $\bm{u}$ on $W$} or \emph{vector component of $\bm{u}$ along $W$}, and the vector $\bm{w_2}$ is called the \emph{vector component of $\bm{u}$ orthogonal to $W$}. Calculating this can be done using the orthogonal or orthonormal basis of $W$, as given in theorem [\ref{thm_projection_formula}]. We also give a method to use any basis on appendix [\ref{fmla_proj_subspace}], although often times using orthonormal basis will yield a more comprehensive understanding of $\bm{u}$ through its coordinates. We now give the following theorem:
		
		\begin{thm}
			Every nonzero finite-dimensional inner product space has an orthonormal basis.
		\end{thm}
		
		We now give a method to convert any given basis of a vector space to an orthogonal(or orthonormal) basis. This process is called the \emph{Gram-Schmidt Process}.
		
		\begin{mthd}[Gram-Schmidt Process]\label{gram_schmidt_process}
			To convert a basis $\{\bm{u_1},\bm{u_2},\dots,\bm{u_n}\}$ into an orthogonal basis $\{\bm{v_1},\bm{v_2},\dots,\bm{v_n}\}$, perform the following computations, where $W_i=span(\{u_k|k \le i\})$:
			\begin{enumerate}
				\item $\bm{v_1}=\bm{u_1}$
				\item $\bm{v_2}=\bm{u_2}-\verb|proj|_{W_1}\bm{u_2}=\bm{u_2}-\frac{\left<\bm{u_2},\bm{v_1}\right>}{\|v_1\|^2}\bm{v_1}$
				\item $\bm{v_3}=\bm{u_3}-\verb|proj|_{W_1}\bm{u_3}=\bm{u_3}-\frac{\left<\bm{u_3},\bm{v_1}\right>}{\|v_1\|^2}\bm{v_1}-\frac{\left<\bm{u_3},\bm{v_2}\right>}{\|v_2\|^2}\bm{v_2}$
				\\ \vdots
			\end{enumerate}
			
			And continue for $n$ steps.
			
			Note that $W_i=span(\{u_k|k \le i\})=span(\{v_k|k \le i\})$.
			
			Optionally, normalize to get the orthonormal basis.
		\end{mthd}
		
		\begin{thm}
			If $W$ is a finite-dimensional inner product space, then:
			\begin{itemize}
				\item Every orthogonal set of nonzero vectors in $W$ can be enlarged to an orthogonal basis for $W$.
				\item Every orthonormal set in $W$ can be enlarged to an orthonormal basis for $W$.
			\end{itemize}
		\end{thm}
	
	\section{Matrix}
		\subsection{Matrices and its operations}
			\begin{defn}[Matrix]
				A \emph{matrix} is a rectangular array of numbers. The numbers in the array are called the \emph{entries} in the matrix.
			\end{defn}
		
			Equality, addition, and subtraction can only be defined on same-sized matrices, and is defined elementwise; scalar multiplication is also defined elementwise.
			
			\begin{defn}[Matrix Multiplication]
				If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, then the \emph{product} $AB$ is the $m \times n$ matrix whose entries are determined as follows:
				The entry of $AB$ on row $i$ and column $j$, multiply the corresponding entries from the row $i$ from $A$ and column $j$ from $B$, then add them all together.
			\end{defn}
		
			Matrices of the same size may be used in a linear combination, just like vectors[\ref{def_linear_combination_vector}].
			
			\begin{defn}[Linear Combination of a Matrix]
				If $A_1, A_2, \dots, A_r$ are matrices of the same size, and if $c_1, c_2, \dots, c_r$ are scalars, then an expression of the form
				\begin{displaymath}
					c_1A_1+c_2A_2+\cdots+c_rA_r
				\end{displaymath}
				is called a \emph{linear combination} of $A_1, A_2, \dots, A_r$ with coefficients $c_1, c_2, \dots, c_r$.
			\end{defn}
			
			\begin{thm}
				If $A$ is an $m \times n$ matrix and if $\bm{x}$ is an $n \times 1$ column vector, then the product $A\bm{x}$ can be expressed as a linear combination of the column vectors of $A$ in which the coefficients are the entries of $\bm{x}$.
			\end{thm}
			
			\begin{defn}[Transpose]
				For any $m \times n$ matrix, then the \emph{transpose} of $A$, denoted by $A^T$, is defined to be the $n \times m$ matrix that results by interchanging the rows and columns of $A$; that is, the first column of $A^T$ is the first row of $A$ and so forth.
			\end{defn}
			
			\begin{defn}[Trace]
				For a square matrix $A$, the \emph{trace} of $A$, denoted $tr(A)$, is defined to be the sum of the entries on the main diagonal of $A$.
			\end{defn}

	\section{Matrices and Vector Spaces}
	\subsection{Fundamental Spaces of a Matrix}
	There are four important vector spaces on any given matrix, which are row, column, null, and left null spaces.
	\begin{defn}[Fundamental Spaces of a Matrix]
		\begin{itemize}
			\item A \emph{Column space} of a matrix, denoted $im(A)$(image of $A$), $range(A)$(range of $A$), $col(A)$ or $C(A)$, is the vectors spanned by the column vectors of the matrix. $dim(col(A))$ is often called the \emph{rank} of $A$, denoted $rank(A)$.
			\item A \emph{Row space} of a matrix, denoted $col(A^T)$, and sometimes called the coimage, is the vectors spanned by the row vectors of the matrix.
			\item A \emph{Null space} of a matrix, denoted $ker(A)$(kernel of $A$), $null(A)$ or $N(A)$, is the vector space of the solution vectors of the equation $A\bm{x}=\bm{0}$. $dim(null(A))$ is often called the \emph{nullity} of $A$, denoted $nullity(A)$.
			\item A \emph{Left Null space} of a matrix, denoted $null(A^T)$, and sometimes called the cokernel, is the vector space of the solutions vectors of the equation $A^T\bm{y}=\bm{0}$.
		\end{itemize}
		The four spaces together are called the fundamental spaces of a matrix.
	\end{defn}
	
	\begin{defn}[Rank of a Matrix]
		The rank of a matrix $A$, denoted $rank(A)$, is defined to be the dimension of the column space.
	\end{defn}
	
	\begin{defn}[Full Rank]
		A matrix is said to have \emph{full rank} if its rank is largest possible among the matrices of the same dimensions, which is the minimum of the number of rows and columns.
	\end{defn}
	
	\begin{thm}
		$rank(A)$ equals the number of nonzero rows in $rref(A)$.
	\end{thm}
	
	From the definitions above, we gain the fundamental theorem of linear algebra.
	
	\begin{thm}[Fundamental Theorem of Linear Algebra, Pt. 1]
		Suppose a matrix A is $m \times n$. Let $r=rank(A)$. Fundamental subspaces of the matrix $A$ has the following dimensions:
		
		\begin{tabular}{ccc}
			\textbf{Name of Subspace} & \textbf{Containing Space} & \textbf{Dimension} \\
			Column Space ($C(A)$)             & $\mathbb{R}^m$            & $rank(A)=r$        \\
			Null Space ($N(A)$)               & $\mathbb{R}^n$            & $nullity(A)=n-r$   \\
			Row Space ($C(A^T)$)              & $\mathbb{R}^n$            & $rank(A)=r$        \\
			Left Nullspace ($N(A^T)$)         & $\mathbb{R}^m$            & $corank(A)=m-r$   
		\end{tabular}
	\end{thm}
	
	\begin{thm}[Fundamental Theorem of Linear Algebra, Pt. 2]
		\begin{itemize}
			\item $N(A)^\perp=C(A^T)$ in $\mathbb{R}^n$, that is, nullspace and row space are orthogonal complements.
			\item $C(A)^\perp=N(A^T)$ in $\mathbb{R}^m$, that is, column space and left null space are orthogonal complements.
		\end{itemize}
	\end{thm}
	
	\subsection{Change of Basis}
	We start from the definition of basis[\ref{def_basis}] and the concept of coordinates[\ref{def_coordinate}]. We assume that the scalar is $\mathbb{R}$ for simplicity, although any other field may be used as a scalar.
	
	Say that we are talking about a general vector space $V$ over a scalar $F$ which has $S=\{\bm{v_1},\bm{v_2},\dots,\bm{v_n}\}$ as its basis, by theorem [\ref{thm_coordinate_unique}], any vector $\bm{v}\in V$ can be represented uniquely as $\bm{v}=c_1\bm{v_1}+c_2\bm{v_2}+\dots+c_n\bm{v_n}$, where $c_i \in F$. Observe that the vector $(\bm{v})_S=(c_1,c_2,\dots,c_n) \in \mathbb{F}^n$, and hence once basis $S$ is given for a vector space $V$, theorem [\ref{thm_coordinate_unique}] ensures that this correspondence between vectors in $V$ and $\mathbb{F}^n$ is one-to-one.
	
	However this is not that simple. Suppose the ordering of the basis vectors is switched via a permutation $\sigma$, so that $S=\{u_i|u_i=v_{\sigma(i)}\}$. Now, the set of the basis stays the same, but $(\bm{v})_S=(c_{\sigma(1)},c_{\sigma(2)},\dots,c_{\sigma(n)})$. In this exact reason, when we determine the coordinates, an ordered set(i.e. a set in which ordering matters) is used. Some authors call a set of basis vectors of which changing the order is restricted an ordered basis. We simply opt to the solution that when discussing a vector space and its basis $S$, the order of the vectors in $S$ remain fixed unless stated otherwise.
	
	In the special case where $V=\mathbb{R}^n$ and $S$ is the \emph{standard basis}, i.e. $S=\{e_1,e_2,\dots,e_n\}$ where $e_i$ has zeroes as all of its components except for the $i$-th component, the coordinate vector $(\bm{v})_S$ and the vector $\bm{v}$ are the same.
	
	If $S=\{v_1,v_2,\dots,v_n\}$ is a basis for a finite-dimensional vector space $V$, and if $(\bm{v})_S=(c_1,c_2,\dots,c_n)$ is the coordinate vector of $\bm{v}$ relative to $S$, then the mapping $\bm{v} \rightarrow (\bm{v})_S$ creates a connection between vectors in the general vector space $V$ and vectors in the vector space $\mathbb{R}^n$, which is more familiar to handle. We call this mapping the \emph{coordinate map} from $V$ to $\mathbb{R}^n$. Since we have all the tools to analyze this when we represent this vector as a matrix, we will be representing this mapping in the matrix form,
	\begin{displaymath}
	[\bm{v}]_S=\begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}
	\end{displaymath}
	where the square brackets simply emphasize the fact that this is in a matrix of a column vector form.
	
	The \emph{Change-of-Basis Problem} states the following:
	
	If $\bm{v}$ is a vector in a finite-dimensional vector space $V$, and if we change the basis for $V$ from a basis $B$ to basis $B'$, how are the coordinate vectors $[\bm{b}]_B$ and $[\bm{b}]_{B'}$ related?
	
	To solve this problem, let:
	\begin{itemize}
		\item $B=\{\bm{u_1},\bm{u_2},\dots,\bm{u_n}\}$
		\item $M=\begin{bmatrix} \bm{u_1} & \bm{u_2} & \dots & \bm{u_n} \end{bmatrix}$
		\item $B'=\{\bm{v_1},\bm{v_2},\dots,\bm{v_n}\}$
		\item $N=\begin{bmatrix} \bm{v_1} & \bm{v_2} & \dots & \bm{v_n} \end{bmatrix}$
		\item $(\bm{b})_B=(c_1,c_2,\dots,c_n)$
		\item $(\bm{b})_{B'}=(d_1,d_2,\dots,d_n)$
	\end{itemize}
	
	We can see from this that $M[\bm{b}]_B=N[\bm{b}]_{B'}=\bm{b}$.
	
	Since the vectors of $B'$ are all in the vector space $V$, we can calculate their coordinates with respect to $B$, so say that $\bm{p_i}=[\bm{v_i}]_B$.
	
	If we consider a matrix given by $P=\begin{bmatrix} \bm{p_1} & \bm{p_2} & \dots & \bm{p_n} \end{bmatrix}$, we can clearly see that $M=NP$. Substitution yields $NP[\bm{b}]_B=N[\bm{b}]_{B'}=\bm{b}$, which indicates, by the uniqueness of coordinates(theorem [\ref{thm_coordinate_unique}]), $[\bm{b}]_{B'}=P[\bm{b}]_B$. The matrix $P$, often denoted $P_{B \rightarrow B'}$ is called the \emph{transition matrix} from $B$ to $B'$.
	
	In words, this can be represented as follows: \textit{The columns of the transition matrix from an old basis to a new basis are the coordinate vectors of the old basis relative to the new basis.}
	
	The following theorem is about the invertibility[\ref{def_invertible_matrix}] of the transition matrix, which is written in a future section.
	
	\begin{thm}
		If $P$ is the transition matrix from a basis $B'$ to a basis $B$ for a finite-dimensional vector space $V$, then $P$ is invertible and $P^{-1}$ is the transition matrix from $B$ to $B'$.
	\end{thm}
	
	We now conclude this section by introducing a procedure for computing $P_{B \rightarrow B'}$:
	\begin{mthd}[Computing a Transition Matrix $P_{B \rightarrow B'}$]
		$B$ and $B'$ are basis for a finite-dimensional vector space $V$.
		\begin{enumerate}[label=Step \arabic*.]
			\item Form the matrix $[B'|B]$.
			\item Use elementary row operations to reduce the matrix to its rref.
			\item The resulting matrix is $[I|P_{B\rightarrow B'}]$.
		\end{enumerate}
	\end{mthd}
	

	\section{Inverse}
		\subsection{Elementary Row Operations and Matrices}
		
		\begin{defn}[Elementary Row Operations]\label{def_elementary_row_operations}
			The following three operations are said to be the \emph{elementary row operations} on a matrix:
			\begin{enumerate}
				\item Multiply a row through by a nonzero constant.
				\item Interchange two rows.
				\item Add a constant times one row to another.
			\end{enumerate}
		\end{defn}
		
		\begin{defn}[Elementary Row Matrices]
			An $n \times n$ matrix is called an \emph{elementary matrix} if it can be obtained from the $n \times n$ identity matrix $I_n$ by performing a single elementary row operation.
		\end{defn}
		
		\begin{thm}[Elementary Row Operations and Elementary Row Matrices]
			If the elementary matrix $E$ results from performing a certain row operation on $I_m$ and $A$ is an $m \times n$ matrix, then the product $EA$ is the matrix that results when this same row operation is performed on $A$.
		\end{thm}
		
		\begin{defn}[Reduced-row Echelon Form]
			A matrix that is in its \emph{reduced-row echelon form(rref)} has the following properties:
			\begin{enumerate}
				\item If a row does not consist entirely of zeroes, then the first nonzero number in the row is a 1. We call this a \emph{leading 1}.
				\item If there are any rows that consist entirely of zeroes, then they are grouped together at the bottom of the matrix.
				\item In any two successive rows that do not consist entirely of zeroes, the leading 1 in the lower row occurs farther to the right than the leading 1 in the higher row.
				\item Each column that contains a leading 1 has zeroes everywhere else in that column
			\end{enumerate}
			A matrix that has the first three properties is said to be in \emph{row echelon form}.
		\end{defn}
		
		\begin{thm}
			If $R$ is the reduced row echelon form of an $n \times n$ matrix $A$, then either $R$ has a row of zeroes or $R$ is the identity matrix $I_n$.
		\end{thm}
		
		There are two important facts on echelon forms:
		\begin{enumerate}
			\item Every matrix has a unique rref.
			\item Row echelon forms are not unique, but, they have the same:
			\begin{itemize}
				\item number of zero rows
				\item positions of leading 1's
					\subitem the positions are called the \emph{pivot positions} of A
					\subitem the columns are called the \emph{pivot column} of A
			\end{itemize}
		\end{enumerate}
		
		\begin{mthd}[Gauss-Jordan Elimination]\label{mthd_gauss_jordan_elim}
			This method will use elementary row operations and through two phases, forward and backward phases, reduces a matrix into its reduced row echelon form.
			\begin{enumerate}[label=Phase \arabic*.]
				\item Forward Phase\footnote{If only this phase is used to produce a row echelon form, this is called the Gaussian elimination.}
				\begin{enumerate}[label=Step \arabic*.]
					\item Locate the leftmost column that does not consist entirely of zeroes.
					\item Interchange the top row with another row, if necessary, to bring a nonzero entry to the top of the column found in Step 1.
					\item Multiply the first row by a constant so that it has a leading 1.
					\item Add suitable multiples of the top row to the rows below so that all entries below the leading 1 become zeroes.
					\item Restart from Step 1, ignoring the upper rows until the entire matrix is in row echelon form.
				\end{enumerate}
				\item Backward Phase
				\begin{enumerate}[label=Step \arabic*.]
					\setcounter{enumii}{6}
					\item Beginning with the last nonzero row and working upward, add suitable multiples of each row to the rows above to make the entries above the leading 1's to 0.
				\end{enumerate}
			\end{enumerate}
		\end{mthd}
		
		\subsection{Finding the Inverse for a Matrix}
		\begin{defn}[Inverse]\label{def_invertible_matrix}
			If $A$ is a square matrix, and if a matrix $B$ of the same size can be found so that $AB=BA=I$, then $A$ is said to be \emph{invertible} or \emph{nonsingular} and $B$ is called an \emph{inverse} of $A$, denoted by $A^{-1}$. If no such matrix $B$ can be found, then $A$ is said to be \emph{singular} or \emph{non-invertible}.
		\end{defn}
		
		\begin{thm}
			If $B$ and $C$ are both inverses of the matrix $A$, then $B=C$.
		\end{thm}
		
		\begin{thm}[Inverse of a 2-by-2 matrix]\label{thm_inv_2_by_2}
			The matrix
			\begin{displaymath}
				A=
				\begin{bmatrix}
					a & b \\ c & d
				\end{bmatrix}
			\end{displaymath}
			is invertible iff $ad-bc\ne0$, in which case the inverse is given by:
			\begin{displaymath}
				A^{-1}=\frac{1}{ad-bc}
				\begin{bmatrix}
					d & -b \\ -c & a
				\end{bmatrix}
			\end{displaymath}
		\end{thm}
		
		\begin{thm}
			If $A$ and $B$ are invertible matrices with the same size, then $AB$ is invertible and $(AB)^{-1}=B^{-1}A^{-1}$.\\
			In general, a product of any number of invertible matrices is invertible, and the inverse of the product is the product of the inverses in the reverse order.
		\end{thm}
		
		\begin{thm}
			If $A$ is invertible, then $A^T$ is also invertible, and $(A^T)^{-1}=(A^{-1})^T$.
		\end{thm}
	
		\begin{thm}\label{thm_ata}
			$A^TA$ is invertible iff the column vectors of $A$ are linearly independent.
		\end{thm}
		
		\begin{thm}\label{thm_elementary_invertible}
			Every elementary matrix is invertible, and the inverse is also an elementary matrix.
		\end{thm}
		
		\begin{mthd}[Inversion Algorithm]
			To find the inverse of an invertible matrix $A$, find a sequence of elementary row operations that reduces $A$ to the identity and then perform that same sequence of operations on $I_n$ to obtain $A^{-1}$.
			
			For easier approach, simply use Gauss-Jordan Elimination[\ref{mthd_gauss_jordan_elim}] to the augmented matrix $\left[A|I_n\right]$ so that it becomes $\left[I_n|A^{-1}\right]$.
		\end{mthd}
		
	\subsection{Matrix Transformations from $\mathbb{R}^n$ to $\mathbb{R}^m$}
	
		Recall that a \emph{function} is a rule that associates with each element of a set $A$ one and only one element in a set $B$. If $f$ associates the element $b \in B$ with $a \in A$, we write $b=f(a)$ and we say that $b$ is the \emph{image} of $a$ under $f$ or that $f(a)$ is the \emph{value} of $f$ at $a$. The set $A$ is called the \emph{domain} of $f$ and the set $B$ the \emph{codomain} of $f$. The set $f(A)=\{f(a)|a \in A\}$ is called the \emph{range} of $f$.
		
		\begin{defn}[Transformation]
			If $V$ and $W$ are vector spaces, and if $f$ is a function with domain $V$ and codomain $W$, then we say that $f$ is a \emph{transformation} from $V$ to $W$ or that $f$ maps $V$ to $W$, which we denote with $f:V \rightarrow W$.
			
			In the special case where $V=W$, $f$ is also called an \emph{operator} on $V$.
		\end{defn}
		
		Since we are talking about matrices, we are going to consider the transformations from $\mathbb{R}^n$ to $\mathbb{R}^m$ which can be represented as a matrix multiplication as follows:
		\begin{displaymath}
			\begin{bmatrix}
				w_1 \\ w_2 \\ \vdots \\ w_m
			\end{bmatrix}
			=
			\begin{bmatrix}
				a_{11} & a_{12} & \dots & a_{1n} \\
				a_{21} & a_{22} & \dots & a_{2n} \\
				\vdots & \vdots &       & \vdots \\
				a_{n1} & a_{n2} & \dots & a_{nn}
			\end{bmatrix}
			\begin{bmatrix}
				x_1 \\ x_2 \\ \vdots \\ x_n
			\end{bmatrix}
		\end{displaymath}
		
		or more briefly as $\bm{w}=A\bm{x}$.
		
		This can be viewed as a linear system, but if we consider $\bm{w}$ as a vector in $\mathbb{R}^m$ and $\bm{x}$ as a vector in $\mathbb{R}^n$, we can see this as a transformation. We call this \emph{matrix transformation}(or \emph{matrix operator} if $m=n$), denoted by $T_A:\mathbb{R}^n\rightarrow \mathbb{R}^m$, and thereby $\bm{w}=T_A(\bm{x})$, or sometimes $\bm{x} \xrightarrow{T_A} \bm{w}$. $T_A$ is called \emph{multiplication by $A$}, and the matrix $A$ is called the \emph{standard matrix} for the transformation.
		
		\begin{thm}
			For every matrix $A$, the matrix transformation $T_A:\mathbb{R}^n \rightarrow \mathbb{R}^m$ has the following properties for all vectors $\bm{u}$ and $\bm{v}$ and for every scalar $k$:
			\begin{enumerate}
				\item $T_A(\bm{0})=\bm{0}$
				\item $T_A(k\bm{u})=kT_A(\bm{u})$ [Homogeniety Property]
				\item $T_A(\bm{u}+\bm{v})=T_A(\bm{u})+T_A(\bm{v})$ [Additivity Property]
				\item $T_A(\bm{u}-\bm{v})=T_A(\bm{u})-T_A(\bm{v})$
			\end{enumerate}
		\end{thm}
		
		\begin{thm}
			If $T_A:\mathbb{R}^n \rightarrow \mathbb{R}^m$ and $T_B:\mathbb{R}^n \rightarrow \mathbb{R}^m$ are matrix transformations, and if $\forall \bm{x} \in \mathbb{R}, T_A(\bm{x})=T_B(\bm{x})$, then $A=B$.
		\end{thm}
		
		\begin{defn}
			If $T_A:\mathbb{R}^n \rightarrow \mathbb{R}^k$ and $T_B:\mathbb{R}^k \rightarrow \mathbb{R}^m$ are matrix transformations, the \emph{composition} of $T_B$ with $T_A$, denoted by $T_B \circ T_A$, is defined by $\bm{x} \xrightarrow{T_B \circ T_A} T_B(T_A(\bm{x}))$.
		\end{defn}
		
		\begin{thm}
			If $T_A:\mathbb{R}^n \rightarrow \mathbb{R}^k$ and $T_B:\mathbb{R}^k \rightarrow \mathbb{R}^m$ are matrix transformations, $T_B \circ T_A=T_{BA}$.
		\end{thm}
		
		\begin{thm}
			$T:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is a matrix transformation iff the following relationships hold $\forall \bm{u}, \bm{v} \in \mathbb{R}^n$ and for every scalar $k$:
			\begin{enumerate}
				\item $T_A(\bm{u}+\bm{v})=T_A(\bm{u})+T_A(\bm{v})$ [Additivity Property]
				\item $T_A(k\bm{u})=kT_A(\bm{u})$ [Homogeniety Property]
			\end{enumerate}
		\end{thm}
		
		The additivity and homogeneity properties in the theorem above are called \emph{linearity conditions}, and a transformation that satisfies these conditions is called a \emph{linear transformation}. Restating the theorem above gives the following:
		
		\begin{thm}
			Every linear transformation from $\mathbb{R}^n$ to $\mathbb{R}^m$ is a matrix transformation and vice versa.
		\end{thm}
	
	\section{Determinants}
		Recall from [\ref{thm_inv_2_by_2}] that the $2 \times 2$ matrix $A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ is invertible iff $ad-bc \ne 0$. The term $ad-bc$ is the determinant of the matrix $A$. \emph{Determinant} is a scalar value that can be computed from the elements of a square matrix which encodes certain properties of the matrix.
		\subsection{Calculating Determinants}
			There are two methods to calculate the determinant.
			\subsubsection{Method of Cofactor Expansion}
				\begin{defn}[Minors and Cofactors]
					Let $A$ be a square matrix. Then the \emph{minor of entry $a_{ij}$}, denoted by $M_ij$, is defined to be the determinant of the submatrix that remains after the i-th row and j-th column are deleted from $A$. The number $C_{ij}=(-1)^{i+j}M_{ij}$ is called the \emph{cofactor of entry $a_{ij}$}.
				\end{defn}
				
				\begin{defn}[Adjoint]
					If $A$ is $n \times n$ matrix and $C_{ij}$ is the cofactor of $a_{ij}$, then the matrix
					\begin{displaymath}
					\begin{bmatrix}
						C_{11} & C_{12} & \cdots & C_{1n} \\
						C_{21} & C_{22} & \cdots & C_{2n} \\
						\vdots & \vdots &        & \vdots \\
						C_{n1} & C_{n2} & \cdots & C_{nn} \\
					\end{bmatrix}
					\end{displaymath}
					is called the \emph{matrix of cofactors from $A$}. The transpose of this matrix is called the \emph{adjoint of $A$}, denoted by $\verb|adj|(A)$.
				\end{defn}
				
				\begin{defn}[Determinant]
					If $A$ is an $n \times n$ matrix, then the number obtained by multiplying the entries in any row or column of $A$ by the corresponding cofactors and adding the resulting products is called the \emph{determinant} of $A$, and the sums themselves are called \emph{cofactor expansions} of $A$.
					
					The cofactor expansion along the j-th column is as follows:
					\begin{displaymath}
						\det(A)=a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}
					\end{displaymath}
					
					and the cofactor expansion along the i-th row is as follows:
					\begin{displaymath}
						\det(A)=a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in}
					\end{displaymath}
				\end{defn}
				
				\begin{thm}\label{thm_triangular_determinant}
					If $A$ is an $n \times n$ triangular matrix, then $\det(A)$ is the product of entries on the main diagonal of the matrix; that is, $\det(A)=a_{11}a_{22}\cdots a_{nn}$.
				\end{thm}
			
			\subsubsection{Method of Elementary Row Operations}
				This section presents a series of theorems that can be proven with the cofactor expansion formula that will suffice by themselves, paired with the theorem for determinants for triangular matrices[\ref{thm_triangular_determinant}](or simply the fact that $\forall n, \det(I_n)=1$), to find the determinant by continuously applying elementary row operations to the target matrix.
				\begin{thm}
					Let $A$ be a square matrix. If $A$ has a row of zeroes or a column of zeroes, then $\det(A)=0$.
				\end{thm}
				
				\begin{thm}
					Let $A$ be a square matrix. Then $\det(A)=\det(A^T)$.
				\end{thm}
				
				\begin{thm}\label{thm_row_ops_det}
					Let $A$ be an $n \times n$ matrix.
					\begin{enumerate}
						\item If $B$ is the matrix that results when a single row or single column or $A$ is multiplies by a scalar $k$, then $\det(B)=k\det(A)$.
						\item If $B$ is the matrix that results when two rows or two columns of $A$ are interchanged, then $\det(B)=-\det(A)$.
						\item If $B$ is the matrix that results when a multiple of one row of $A$ is added to another row or when a multiple of one column is added another column, then $\det(B)=\det(A)$.
					\end{enumerate}
				\end{thm}
				
				\begin{coro}
					Let $E$ be an $n \times n$ matrix.
					\begin{enumerate}
						\item If $E$ results from multiplying a row of $I_n$ by a nonzero number $k$, then $\det(E)=k$.
						\item If $E$ results from interchanging two rows of $I_n$, then $det(E)=-1$.
						\item If $E$ results from adding a multiple of one row of $I_n$ to another, then $\det(E)=1$.
					\end{enumerate}
				\end{coro}
				
				\begin{thm}
					If $A$ is a square matrix with two proportional rows or two proportional columns, then $\det(A)=0$.
				\end{thm}
				
				Often times, the method of elementary row operations may be applied partially to assist with cofactor expansion formula.
			
		\subsection{Properties of Determinants}
			\begin{thm}
				Let $A$, $B$, and $C$ be $n \times n$ matrices that differ only in a single row, say the r-th row, and assume that the r-th row of $C$ can be obtained by adding corresponding entries in the r-th row of $A$ and $B$. Then, $\det(C)=\det(A)+\det(B)$.
				
				The same result holds for columns.
			\end{thm}
			
			Pairing the theorem above with theorem \ref{thm_row_ops_det}'s first fact, we can say that the determinant is a linear function of each row separately.
			
			\begin{thm}
				A square matrix $A$ is invertible iff $\det(A) \ne 0$.
			\end{thm}
			
			\begin{thm}
				If $A$ and $B$ are square matrices of the same size, then $\det(AB)=\det(A)\det(B)$.
			\end{thm}
			
			\begin{thm}
				If $A$ is invertible, then
				\begin{displaymath}
					\det(A^{-1})=\frac{1}{\det(A)}
				\end{displaymath}
			\end{thm}
			
			\begin{thm}[Inverse of a Matrix using its Adjoint]
				If $A$ is an invertible matrix, then
				\begin{displaymath}
					A^{-1}=\frac{1}{\det(A)}\verb|adj|(A)
				\end{displaymath}
			\end{thm}
			
			\begin{thm}[Cramer's Rule]
				If $A\bm{x}=\bm{b}$ is a system of n linear equations such that $det(A)\ne0$, then the system has a unique solution. The solution is:
				\begin{displaymath}
					\forall i, x_i=\frac{\det(A_i)}{\det(A)}
				\end{displaymath}
				where $A_i$ is the matrix obtained by replacing the entries in the j-th column of $A$ by the entries in the matrix $\bm{b}$.
			\end{thm}
		
	\section{Eigenvalues and Eigenvectors}
		\subsection{Characteristic Polynomial}
			\begin{defn}[Characteristic Polynomial]\label{def_characteristic_polynomial}
				The \emph{characteristic polynomial} of an $n \times n$ square matrix $A$ is defined to be
				\begin{displaymath}
					p_A(t)=\det(tI-A)
				\end{displaymath}
			\end{defn}
			To name some of the properties of any given characteristic polynomial themselves, they are monic(meaning that the leading coefficient is 1), and has degree $n$.
			
			The characteristic polynomial encodes many properties of the matrix. The most obvious property would be $p_A(t)=\det(A)$, and a slightly less obvious one would be that the coefficient of the term $t^{n-1}$ equals tr$(A)$.
			
			You might be wondering, "Hey, why is this book talking about some kind of polynomial in a chapter about eigenvalues and eigenvectors?" Well, that'll become obvious once you see the definition of eigenvalues and eigenvectors.
		
		\subsection{Eigenvalues and Eigenvectors}
		\begin{defn}[Eigenvalues and Eigenvectors]
			If $A$ is an $n \times n$ matrix, then a nonzero vector $\bm{x}$ in $\mathbb{R}^n$ is called an \emph{eigenvector} of $A$ if $A\bm{x}=\lambda \bm{x}$ for some $\lambda \in \mathbb{R}$. $\lambda$ is called an \emph{eigenvalue} of $A$, and $\bm{x}$ is said to be an \emph{eigenvector corresponding to $\lambda$}.
		\end{defn}
		
		Now since $A\bm{x}=\lambda \bm{x}=\lambda I_n \bm{x}$, it follows that $(\lambda I_n - A)\bm{x}=\bm{0}$. Hey, haven't we seen that equation before for the characteristic polynomial[\ref{def_characteristic_polynomial}]? We can change this equation into $p_A(\lambda)=0$, therefore the following theorem:
		
		\begin{thm}
			For an $n \times n$ matrix A, $\lambda$ is an eigenvalue of $A$ iff $p_A(\lambda)=\det(\lambda I-A)=0$.
		\end{thm}
		
		Now, visiting the
		\ifcsname r@thm_fundamental_theorem_of_algebra \endcsname
		Fundamental Theorem of Algebra[\ref{thm_fundamental_theorem_of_algebra}],
		\else
		\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{Fundamental Theorem of Algebra},
		\fi
		we can see that there are exactly $n$ (possibly complex and multiple) roots of a characteristic polynomial of $n \times n$ matrix, and therefore can have up to $n$ distinct eigenvalues.
		
		\begin{thm}
			If $A$ is an $n \times n$ matrix, the following statements are equivalent:
			\begin{enumerate}
				\item $\lambda$ is an eigenvalue of $A$
				\item The system of equations $(\lambda I-A)\bm{x}=\bm{0}$ has nontrivial solutions
				\item There is a nonzero vector $\bm{x}$ such that $A\bm{x}=\lambda \bm{x}$
				\item $\lambda$ is a solution of the characteristic equation $\det(\lambda I-A)=0$
			\end{enumerate}
		\end{thm}
		
		Since the eigenvectors corresponding to an eigenvalue $\lambda$ of a matrix $A$  are the nonzero vectors that satisfy the equation $(\lambda I-A)\bm{x}=\bm{0}$, these eigenvectors are the nonzero vectors in the null space of the matrix $\lambda I-A$. We call this null space the \emph{eigenspace} of $A$ corresponding to $\lambda$.
		
		\begin{thm}
			If $k$ is a positive integer, $\lambda$ is an eigenvalue of a matrix $A$, and $\bm{x}$ is a corresponding eigenvector, then $\lambda^k$ is an eigenvalue of $A^k$ and $\bm{x}$ is a corresponding eigenvector.
		\end{thm}
		
		\begin{thm}
			A square matrix $A$ is invertible iff $\lambda=0$ is not an eigenvalue of $A$.
		\end{thm}
	
	\section{Special Matrices}
		\subsection{Diagonal Matrices}
		A \emph{diagonal matrix} is a square matrix in which all entries off the main diagonal are zero. They can be represented in the following form:
		
		\begin{displaymath}
		D=
		\begin{bmatrix}
		d_1 & 0 & \cdots & 0 \\
		0 & d_2 & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n \\
		\end{bmatrix}
		\end{displaymath}
		
		A diagonal matrix is invertible iff all of its diagonal entries are nonzero, and its inverse is:
		
		\begin{displaymath}
		D^{-1}=
		\begin{bmatrix}
		d_1^{-1} & 0 & \cdots & 0 \\
		0 & d_2^{-1} & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n^{-1} \\
		\end{bmatrix}
		\end{displaymath}
		
		It is easy to calculate powers of diagonal matrices. More specifically,
		
		\begin{displaymath}
		D^{k}=
		\begin{bmatrix}
		d_1^{k} & 0 & \cdots & 0 \\
		0 & d_2^{k} & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n^{k} \\
		\end{bmatrix}
		\end{displaymath}
		
		\subsection{Triangular Matrices}
		A \emph{lower triangular} matrix is a matrix in which all the entries above the main diagonal are zero; an \emph{upper triangular} matrix is a matrix in which all the entries below the main diagonal are zero. Either of they are called \emph{triangular}. They can be represented in the following form:
		
		\begin{displaymath}
		L=
			\begin{bmatrix}
				l_{11}     & 0          & \cdots & 0              & 0      \\
				l_{21}     & l_{22}     & \cdots & 0              & 0      \\
				\vdots     & \vdots     &        & \vdots         & \vdots \\
				l_{(n-1)1} & l_{(n-1)2} & \cdots & l_{(n-1)(n-1)} & 0      \\
				l_{n1}     & l_{n2}     & \cdots & l_{n(n-1)}     & l_{nn} \\
			\end{bmatrix}
		, U=
			\begin{bmatrix}
				u_{11} & u_{12} & \cdots & u_{1(n-1)}     & u_{1n}     \\
				0      & u_{22} & \cdots & u_{2(n-1)}     & u_{2n}     \\
				\vdots & \vdots &        & \vdots         & \vdots     \\
				0      & 0      & \cdots & u_{(n-1)(n-1)} & u_{(n-1)n} \\
				0      & 0      & \cdots & 0              & u_{nn}     \\
			\end{bmatrix}
		\end{displaymath}
		
		\begin{thm}
			\begin{enumerate}
				\item The transpose of a lower triangular matrix is upper triangular, and vice versa.
				\item The product of lower triangular matrices is lower triangular, and same for the upper triangular matrices.
				\item A triangular matrix is invertible iff its diagonal entries are all nonzero.
				\item The inverse of an invertible lower triangular matrix is lower triangular, and same for the invertible upper triangular matrices.
			\end{enumerate}
		\end{thm}
		
		\subsection{Symmetric Matrices}
		A \emph{symmetric} matrix is a square matrix such that $S=S^T$. Specifically, $S$ is symmetric iff $\forall 1 \le i,j \le n, S_{ij}=S_{ji}$.
		
		It is important to note that for two symmetric matrices $A$ and $B$, $(AB)^T=B^TA^T=BA$, and therefore their product is not guaranteed to be symmetric unless $AB=BA$, that is, $A$ and $B$ commute.
		
		\begin{thm}
			The product of two symmetric matrices is symmetric iff the matrices commute.
		\end{thm}
		
		In general, a symmetric matrix may not be invertible. However if they are, the following theorem shows an interesting fact:
		
		\begin{thm}
			If $A$ is an invertible symmetric matrix, then $A^{-1}$ is symmetric.
		\end{thm}
		
		\subsection{Similar Matrices}
		\begin{defn}[Similar Matrices]
			If $A$ and $B$ are square matrices, then we say that \emph{$B$ is similar to $A$} or \emph{$A$ and $B$ are similar matrices} if there is an invertible matrix $P$ such that $B=P^{-1}AP$
		\end{defn}
		
		\begin{thm}[Invariants of Similar Matrices]
			Suppose $A$ and $B$ are similar matrices. The following properties are the same for $A$ and $B$: \\
			\begin{tabular}{ll}
				\multicolumn{1}{c}{\textbf{Property}} & \multicolumn{1}{c}{\textbf{Description}}             \\
				Determinant                           & $\det(A)=\det(B)$                                    \\
				Invertibility                         & $\exists A^{-1}$ iff $\exists B^{-1}$                \\
				Rank                                  & $rank(A)=rank(B)$                                    \\
				Nullity                               & $nullity(A)=nullity(B)$                              \\
				Trace                                 & $tr(A)=tr(B)$                                        \\
				Characteristic Polynomial             & $p_A(t)=p_B(t)$                                      \\
				Eigenvalues                           & $p_A(\lambda)=0$ iff $p_B(\lambda)=0$                \\
				Eigenspace dimension                  & $dim(null(\lambda I-A))=dim(null(\lambda I-A))$
			\end{tabular}
		\end{thm}
		
		\subsection{Positive Definite Matrices}%TODO
		
	
	\section{Preprocessing Matrices for Easier Computation}
	In this section we see methods for to preprocess matrices to enable easier and faster computation, including solving linear equations and calculating the power of a square matrix. This is often called a \emph{decomposition} or \emph{factorization} of a matrix.
	
		\subsection{LU-decomposition}
			\emph{LU-decomposition} is a process of which we factorize a matrix $A$ into two matrices, $A=LU$, of which $L$ is a lower triangular matrix and $U$ is an upper triangular matrix.
		
			We modify Gaussian Elimination[\ref{mthd_gauss_jordan_elim}]'s Forward Phase, of which we do not reorder the rows while we eliminate the matrix into its echelon form(or in this case, upper triangular matrix). Then we gain the following form:
			\begin{displaymath}
				(E_r\dots E_2E_1)A=U
			\end{displaymath}
			Now using theorem \ref{thm_elementary_invertible}, we know that the elementary matrices are invertible, and hence we gain the following:
			\begin{displaymath}
				A=(E_r\dots E_2E_1)^{-1}U=E_1^{-1}E_2^{-1}\dots E_r^{-1}U
			\end{displaymath}
			In the forward phase of the Gaussian Elimination, since we eliminate the possibility of permutation, all the elementary matrices will either be a constant multiple of a row, or subtraction of a multiple of an upper row from a lower row. We now give the following theorem:
			\begin{thm}
				If $A$ is a square matrix that can be reduced to a row echelon form $U$ by Gaussian Elimination without row interchanges, then $A$ can be factored as $A=LU$, where $L$ is a lower triangular matrix.
			\end{thm}
			
			There are two major variants of LU-decomposition.
			
			First, note that LU-decomposition is not unique, since multiplying a nonzero $k$ to column $i$ in $L$ and dividing by $k$ to the row $i$ in $U$ will still give the multiplication result to be $A$. To solve this problem, we restrict the diagonal entries of the matrices $L$ and $U$ to all be ones, and introduce a diagonal matrix $D$ to the mix. By making this a three-matrix factorization, i.e. $A=LDU$, this is a unique decomposition. This is called the \emph{LDU-decomposition}.
			
			Second, we did not allow any row exchanges, but row exchanges are sometimes performed in computer algorithms to reduce roundoff errors that occur due to floating-point arithmetic. By allowing row exchanges, we can alter the decomposition to be $QA=LU$, where $Q$ is a permutation matrix. It is common to express this as $A=PLU$, where $P=Q^{-1}$, which is called the \emph{PLU-decomposition} of $A$.
		
		\subsection{QR-Decomposition}
			We start with a matrix with linearly independent columns(hence, invertible) $A$. Since $A$'s columns are linearly independent, we can apply the Gram-Schmidt Process[\ref{gram_schmidt_process}] to orthonormalize its column vectors to, say a matrix $Q$.
			
			What we need to think about is how the columns of $A$(say $\bm{a_1}, \bm{a_2}, \dots, \bm{a_n}$) relate with columns of $Q$(say $\bm{q_1}, \bm{q_2}, \dots, \bm{q_n}$). If we follow the Gram-Schmidt Process, $\bm{a_k}$ can be represented as a linear combination of the vectors in the set $Q_k\{\bm{q_i}|1 \le i \le k\}$. Considering the fact that the projection of $\bm{a_i}$ onto the space $span(Q_k)$, according to the projection formula[\ref{thm_projection_formula}], is:
			\begin{displaymath}
				\sum_{i=1}^{n}\left<\bm{q_i},\bm{a_k}\right>q_i=\sum_{i=1}^{k}\left<\bm{q_i},\bm{a_k}\right>q_i
			\end{displaymath}
			Equality holds as $\bm{q_\alpha}$ and $\bm{a_\beta}$ are orthogonal if $\alpha>\beta$. Therefore the relationship between $A$ and $Q$ can be represented as follows:
			\begin{displaymath}
				A
				=
				Q
				\begin{bmatrix}
					\left<\bm{q_1}, \bm{a_1}\right> & \left<\bm{q_1}, \bm{a_2}\right> & \cdots & \left<\bm{q_1}, \bm{a_n}\right> \\
					\left<\bm{q_2}, \bm{a_1}\right> & \left<\bm{q_2}, \bm{a_2}\right> & \cdots & \left<\bm{q_2}, \bm{a_n}\right> \\
					\vdots                          & \vdots                          &        & \vdots                          \\
					\left<\bm{q_n}, \bm{a_1}\right> & \left<\bm{q_n}, \bm{a_1}\right> & \cdots & \left<\bm{q_n}, \bm{a_n}\right> \\
				\end{bmatrix}
				=
				Q
				\begin{bmatrix}
				\left<\bm{q_1}, \bm{a_1}\right> & \left<\bm{q_1}, \bm{a_2}\right> & \cdots & \left<\bm{q_1}, \bm{a_n}\right> \\
				0                               & \left<\bm{q_2}, \bm{a_2}\right> & \cdots & \left<\bm{q_2}, \bm{a_n}\right> \\
				\vdots                          & \vdots                          &        & \vdots                          \\
				0                               & 0                               & \cdots & \left<\bm{q_n}, \bm{a_n}\right> \\
				\end{bmatrix}
			\end{displaymath}
			or, $A=QR$. Note that $Q$ is an orthogonal matrix derived from Gram-Schmidt process and $R$ is an upper triangular matrix.
		
		\subsection{Diagonalization of a Matrix}
			\begin{defn}[Diagonalizablility]
				A square matrix $A$ is said to be \emph{diagonalizable} if it is similar to some diagonal matrix; that is, if there exists an invertible matrix $P$ such that $P^{-1}AP$ is diagonal. In this case the matrix $P$ is said to \emph{diagonalize} $A$.
			\end{defn}
			
			\begin{thm}
				If $A$ is an $n \times n$ matrix, $A$ is diagonalizable iff $A$ has $n$ linearly independent eigenvectors.
			\end{thm}
			
			\begin{mthd}[Procedure for Diagonalizing a Matrix]
				We assume that $A$ is an $n \times n$ matrix with $n$ linearly independent eigenvectors.
				\begin{enumerate}
					\item Find the eigenvalues $\lambda_i$ with their corresponding eigenvectors $\bm{p_i}$. This is the step where you can verify that this matrix is indeed diagonalizable.
					\item Form the matrix $P=[\bm{p_1} | \bm{p_2} | \dots | \bm{p_n}]$.
					\item The matrix $P^{-1}AP$ will be diagonal and have the eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$ corresponding to the eigenvectors $\bm{p_1},\bm{p_2},\dots,\bm{p_n}$ as its successive diagonal entries.
				\end{enumerate}
			\end{mthd}
			
			To help verify that the matrix is indeed diagonalizable, we give the following theorem:
			
			\begin{thm}
				If $\bm{v_1},\bm{v_2},\dots,\bm{v_n}$ are eigenvectors of a matrix $A$ corresponding to distinct eigenvalues, then $\{\bm{v_1},\bm{v_2},\dots,\bm{v_n}\}$ is a linearly independent set.
			\end{thm}
			
			and therefore follows the following theorem:
			
			\begin{thm}
				If an $n \times n$ matrix $A$ has $n$ distinct eigenvalues, then $A$ is diagonalizable.
			\end{thm}
		
		\subsection{Singular Value Decomposition(SVD)}%TODO
	
	\section{Solving Linear Equations}
	We come to this final section, the ultimate target of linear algebra: solving a system of linear equations.
		\subsection{Linear Equations to Matrices}
		A finite set of linear equations is called a \emph{system of linear equations}, or more briefly, a \emph{linear system}. The variables are called \emph{unknowns}.
		
		\begin{center}
			\begin{tabular}{ccccccccc}
				$a_{11}x_1$ & $+$ & $a_{12}x_2$ & $+$ & $\cdots$ & $+$ & $a_{1n}x_n$ & $=$ & $b_1$    \\
				$a_{21}x_1$ & $+$ & $a_{22}x_2$ & $+$ & $\cdots$ & $+$ & $a_{2n}x_n$ & $=$ & $b_2$    \\
				$\vdots$    &     & $\vdots$   &     &          &     & $\vdots$    &     & $\vdots$ \\
				$a_{m1}x_1$ & $+$ & $a_{m2}x_2$ & $+$ & $\cdots$ & $+$ & $a_{mn}x_n$ & $=$ & $b_m$   
			\end{tabular}
		\end{center}
		
		A \emph{solution} of a linear system in $x_1,x_2,\dots,x_n$ is a sequence of $n$ numbers $s_1,s_2,\dots,s_n$ for which the substitution $x_i=s_i$ makes each equation a true statement.
		
		We say that a linear system is \emph{consistent} if it has at least one solution and \emph{inconsistent} if it has no solutions.
		
		\begin{thm}
			A system of linear equations has zero, one, or infinitely many solutions. There are no possibilities.
		\end{thm}
		
		If a linear system has infinitely many solutions, then a set of parametric equations from which all solutions can be obtained by assigning numerical values to the parameters is called a \emph{general solution} of the system.
		
		If all constant terms are zero, that is, $\forall i, b_i=0$, it is said to be \emph{homogeneous}. A homogeneous system of linear equations always is consistent since it has $\forall i, x_i=0$ as its solution: this is called the \emph{trivial solution}. If there are other solutions, they are called the \emph{nontrivial solution}.
		
		The system of linear equations above can be represented in a matrix multiplication as shown below:
		
		\begin{displaymath}
			\begin{bmatrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				a_{21} & a_{22} & \cdots & a_{2n} \\
				\vdots & \vdots &        & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn} \\
			\end{bmatrix}
			\begin{bmatrix}
				x_1 \\ x_2 \\ \vdots \\ x_n
			\end{bmatrix}
			=
			\begin{bmatrix}
				b_1 \\ b_2 \\ \vdots \\ b_m
			\end{bmatrix}
		\end{displaymath}
		
		By designating the three matrices $A$, $\bm{x}$ and $\bm{b}$ respectively, we can say that $A\bm{x}=\bm{b}$. In this equation, $A$ is called the \emph{coefficient matrix} of the system.
		
		The \emph{augmented matrix} for the system is obtained by adjoining $\bm{b}$ to $A$ as the last column as follows:
		
		\begin{displaymath}
			\left[\begin{array}{cccc|c}
				a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
				a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
				\vdots & \vdots &        & \vdots & \vdots\\
				a_{m1} & a_{m2} & \cdots & a_{mn} & b_m\\
			\end{array}\right]
		\end{displaymath}
		
		Note the correspondence between basic algebraic operations on a given set of linear systems and elementary row operations on the augmented matrix of the said systems. In the order in the definition [\ref{def_elementary_row_operations}], the correspondences are:
		
		\begin{enumerate}
			\item Multiply an equation through by a nonzero constant
			\item Interchange two equations	
			\item Add a constant times one equation to another.
		\end{enumerate}
		
		By applying elementary row operations to the augmented matrix, we can get to the point where the augmented matrix is reduced to its reduced row echelon form. The variables corresponding to the leading 1's in the augmented matrix is called the \emph{leading variables}. The remaining variables are called \emph{free variables}.
		
		There is an important theorem regarding the number of free variables and homogeneous systems:
		
		\begin{thm}[Free Variable Theorem for Homogeneous Systems]
			If a homogeneous linear system has $n$ unknowns, and if the rref of its augmented matrix has $r$ nonzero rows, then the system has $n-r$ free variables.
		\end{thm}
		
		\begin{coro}
			A homogeneous linear system with more unknowns than equations has infinitely many solutions.
		\end{coro}
		
		In the following sections on finding solutions or parametric equation for solutions where it applies, the coefficient matrix will be noted as $A$, the vector of variables will be noted as $\bm{x}$ and the variables as $x_1, x_2, \dots, x_n$, and the vector for the constants as $\bm{b}$.
		
		\subsection{Method of Inverses}
		
		This can be used iff $A$ is an invertible matrix.
		
		Find the inverse of $A$, $A^{-1}$.
		The only possible solution is $\bm{x}=A^{-1}\bm{b}$.
		
		\subsection{Method of LU-decomposition}

			This can be used for matrices which are LU-factorizable, i.e. we can use this if we can apply Gaussian Elimination without any row exchanges.
			
			\begin{enumerate}
				\item We first decompose $A=LU$. The system in question then becomes $LU\bm{x}=\bm{b}$.
				\item Let $\bm{y}=U\bm{x}$, where $\bm{y}=\begin{bmatrix}y_1 & y_2 & \dots & y_n \end{bmatrix}^T$. The system in question then becomes $L\bm{y}=\bm{b}$.
				\item We know that $L$ is a lower triangular matrix; hence we can simply use front-substitution to find $y_i$ in order.
				\item Now we have the equation $U\bm{x}=\bm{y}$, of which $\bm{y}$ is known. Since $U$ is an upper triangular matrix, we can use back-substitution to find $x_i$ in reverse order.
			\end{enumerate}
		
		\subsection{Method of RREF}\label{mthd_rref}
		
		This can be used for any matrix $A$.
		
		\begin{enumerate}
			\item Reduce the augmented matrix $\left[A|\bm{b}\right]$ to its RREF $\left[R|\bm{c}\right]$
			\item See if $R$ has a zero row. If any of the value of $\bm{c}$ corresponding to the zero row is nonzero, the system is inconsistent.
			\item Exchange the free variables with parametric variables.
			\item Transpose the free variables to RHS so the leading variables(the pivots) are the only ones left on the LHS.
			\item The resulting expressions are the parametric equation for solutions.
		\end{enumerate}
		
		\subsection{Method of Particular and Special Special Solutions}
			
			This can be used for any matrix $A$.
			
			This method is extremely similar to Method of RREF[\ref{mthd_rref}].
			
			In this method, we first find the nullspace of $A$.
			
			\begin{thm}
				If $A$ is an $m \times n$ matrix, then the solution set of the homogeneous linear system $A\bm{x}=\bm{0}$ consists of all vectors in $\mathbb{R}^n$ that are orthogonal to every row vector of $A$.
			\end{thm}
			
			\begin{thm}
				The general solution of a consistent linear system $A\bm{x}=\bm{b}$ can be obtained by adding any specific solution of $A\bm{x}=\bm{b}$ to the general solution of $A\bm{x}=\bm{0}$.
			\end{thm}
			
			The theorem above indicates that we need to find the nullspace of $A$ along with a specific solution of $A\bm{x}=\bm{b}$ to find the whole, general solution of $A\bm{x}=\bm{b}$.
			
			Using Gauss-Jordan Elimination[\ref{mthd_gauss_jordan_elim}], we reduce the augmented matrix $\left[A|\bm{b}\right]$ to its rref, and detect if there are any inconsistencies. This corresponds to the first step on Method of RREF.
			
			First, we find the nullspace for $A$. We get the basis vectors from the rref of $A$. This corresponds to the second and third steps on Method of RREF.
			
			Next, we find the specific solution of $A\bm{x}=\bm{b}$. From the rref of $\left[A|\bm{b}\right]$ say $\left[R|\bm{c}\right]$, solve the equation by setting all free variables to 0. In doing so, since all leading variables have coefficient 1, the values of $c$ immediately correspond to the specific solution of the leading variables. This process is therefore almost automatic.
			
			Now we have the nullspace of $A$ and the specific solution of $A\bm{x}=\bm{b}$; add those two together to gain the whole solution. This corresponds to the fourth and final steps on Method of RREF.
			
		\subsection{Least Squares Approximation}%TODO
			Sometimes there might not exist any solution for a given linear system. In this case, we have no choice but to find the best approximation of the linear system.
		
	
	Before ending this chapter, we summarize this chapter by gathering all the facts on invertible matrices, written in the appendix[\ref{equiv_invert}].
\end{document}

%Introduction to Linear Algebra 4th ed, Gilbert Strang