\documentclass{report}

\begin{document}
The target of Linear Algebra is to solve a system of homogenous linear equations. To do so, we deal with vectors and matrices.
	\section{Vector Spaces}
		For the definitions on vector spaces, subspaces, and bases, refer to the chapter \ref{chap_vector_space}.
		\subsection{Linear Independence}
		We now define linear independence, one of the most important concepts utilized in linear algebra.
		\begin{defn}[Linear Independence]
			if $S=\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_r}\}$ is a nonempty set of vectors in a vector space $V$, then the vector equation $k_1\vec{v_1}+k_2\vec{v_2}+\dots+k_r\vec{v_r}=\vec{0}$ has at least one solution, namely, $k_1=0, k_2=0, \dots, k_r=0$, the \emph{trivial solution}. If this is the only solution, then $S$ is said to be a \emph{linearly independent set}. If there are solutions in addition to the trivial solution, then $S$ is said to be \emph{linearly dependent}.
		\end{defn}
		
		\begin{thm}
			Let $S=\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_r}\}$ be a set of vectors in $\mathbb{R}^n$. If $r>n$, then $S$ is linearly dependent.
		\end{thm}
		
		\subsection{Orthogonality}
		\begin{defn}[Euclidean Inner Product]
			Let $\vec{u}=(u_1,u_2,\dots,u_n)$ and $\vec{v}=(v_1,v_2,\dots,v_n)$ in $\mathbb{R}^n$. The inner product of the two vectors $\vec{u}$ and $\vec{v}$ is defined as
			\begin{displaymath}
				\vec{u}\cdot\vec{v}=\sum_{i=1}^{n}u_iv_i=u_1v_1+u_2v_2+\dots+u_nv_n
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Norm]
			The \emph{norm} of $\vec{u}=(u_1,u_2,\dots,u_n)$ in $\mathbb{R}^n$, denoted $\|\vec{u}\|$, is defined by
			\begin{displaymath}
			\|\vec{u}\|=\sqrt{\vec{u} \cdot \vec{u}}=\sqrt{\sum_{i=1}^{n}u_i^2}=\sqrt{u_1^2+u_2^2+\dots+u_n^2}
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Unit vectors]
			A vector $\vec{u}$ in $\mathbb{R}^n$ is said to be a unit vector iff $\|\vec{u}\|=1$.
		\end{defn}
		
		\begin{defn}[Angle]
			The \emph{angle} between two nonzero vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ is defined by
			\begin{displaymath}
				\theta=\cos^{-1}(\frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|})
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Orthogonal Vectors]
			Two nonzero vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ are said to be \emph{orthogonal} or \emph{perpendicular} if $\vec{u} \cdot \vec{v} = 0$.
		\end{defn}
		
		\begin{defn}[Orthogonal set]
			A nonempty set of vectors in $\mathbb{R}^n$ is called an \emph{orthogonal set} if all pairs of distinct vectors in the set are orthogonal. If they are also all unit vectors, it is called an \emph{orthonormal set}.\\
			In other words, for a set $\{u_1, u_2, \dots, u_n\}$ to be orthogonal:
			\begin{displaymath}
				u_i \cdot u_j
				\begin{cases}
					\|u_i\|^2 & i=j\\
					0 & i \ne j
				\end{cases}
			\end{displaymath}
			And for the set to be orthonormal, in addition to above, $\forall i \in \{1, 2, \dots n\}, \|u_i\|=1$.
		\end{defn}
	\section{Matrix}
		\subsection{Matrices and its operations}
			\begin{defn}[Matrix]
				A \emph{matrix} is a rectangular array of numbers. The numbers in the array are called the \emph{entries} in the matrix.
			\end{defn}
			Equality, addition, and subtraction can only be defined on same-sized matrices, and is defined elementwise; scalar multiplication is also defined elementwise.
			\begin{defn}[Matrix Multiplication]
				If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, then the \emph{product} $AB$ is the $m \times n$ matrix whose entries are determined as follows:
				The entry of $AB$ on row $i$ and column $j$, multiply the corresponding entries from the row $i$ from $A$ and column $j$ from $B$, then add them all together.
			\end{defn}
			Matrices of the same size may be used in a linear combination, just like vectors[\ref{def_linear_combination_vector}].
			\begin{defn}[Linear Combination of a Matrix]
				If $A_1, A_2, \dots, A_r$ are matrices of the same size, and if $c_1, c_2, \dots, c_r$ are scalars, then an expression of the form
				\begin{displaymath}
					c_1A_1+c_2A_2+\cdots+c_rA_r
				\end{displaymath}
				is called a \emph{linear combination} of $A_1, A_2, \dots, A_r$ with coefficients $c_1, c_2, \dots, c_r$.
			\end{defn}
			\begin{thm}
				If $A$ is an $m \times n$ matrix and if $\vec{x}$ is an $n \times 1$ column vector, then the product $A\vec{x}$ can be expressed as a linear combination of the column vectors of $A$ in which the coefficients are the entries of $\vec{x}$.
			\end{thm}
			\begin{defn}[Transpose]
				For any $m \times n$ matrix, then the \emph{transpose} of $A$, denoted by $A^T$, is defined to be the $n \times m$ matrix that results by interchanging the rows and columns of $A$; that is, the first column of $A^T$ is the first row of $A$ and so forth.
			\end{defn}
			\begin{defn}[Trace]
				For a square matrix $A$, the \emph{trace} of $A$, denoted $tr(A)$, is defined to be the sum of the entries on the main diagonal of $A$.
			\end{defn}
	\section{Inverse}
		\subsection{}
	\section{Determinants}
		\subsection{}
	\section{Eigenvalues and Eigenvectors}
		\subsection{}
	\section{Solving Linear Equations}
	We come to this final section, the ultimate target of linear algebra: solving a system of linear equations.
		\subsection{Linear Equations to Matrices}
		\begin{defn}[Homogeneous Linear Equation]
			A \emph{homogeneous linear equation} in the variables $x_1,x_2,\dots,x_n$ has the form $a_1x_1+a_2x_2+\dots+a_nx_n=0$, where $a_1a_2\dots a_n\ne0$.
		\end{defn}
		A finite set of linear equations is called a \emph{system of linear equations}, or more briefly, a \emph{linear system}. The variables are called \emph{unknowns}. A \emph{solution} of a linear system in $x_1,x_2,\dots,x_n$ is a sequence of $n$ numbers $s_1,s_2,\dots,s_n$ for which the substitution $x_i=s_i$ makes each equation a true statement.
		
		The system of $m$ linear equations in $n$ unknowns,\\
		\begin{center}
		\begin{tabular}{ccccccccc}
			$a_{11}x_1$ & $+$ & $a_{12}x_2$ & $+$ & $\cdots$ & $+$ & $a_{1n}x_n$ & $=$ & $b_1$    \\
			$a_{21}x_1$ & $+$ & $a_{22}x_2$ & $+$ & $\cdots$ & $+$ & $a_{2n}x_n$ & $=$ & $b_2$    \\
			$\vdots$    &     & $\vdots$   &     &          &     & $\vdots$    &     & $\vdots$ \\
			$a_{m1}x_1$ & $+$ & $a_{m2}x_2$ & $+$ & $\cdots$ & $+$ & $a_{mn}x_n$ & $=$ & $b_m$   
		\end{tabular}
		\end{center}
		can be represented in matrices as follows:
		\begin{displaymath}
			\begin{bmatrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				a_{21} & a_{22} & \cdots & a_{2n} \\
				\vdots & \vdots &        & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn} \\
			\end{bmatrix}
			\begin{bmatrix}
				x_1 \\ x_2 \\ \vdots \\ x_n
			\end{bmatrix}
			=
			\begin{bmatrix}
				b_1 \\ b_2 \\ \vdots \\ b_m
			\end{bmatrix}
		\end{displaymath}
		By designating the three matrices $A$, $\vec{x}$ and $\vec{b}$ respectively, we can say that $A\vec{x}=\vec{B}$. In this equation, $A$ is called the \emph{coefficient matrix} of the system.
		
		The \emph{augmented matrix} for the system is obtained by adjoining $\vec{b}$ to $A$ as the last column as follows:
		\begin{displaymath}
			\left[\begin{array}{cccc|c}
				a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
				a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
				\vdots & \vdots &        & \vdots & \vdots\\
				a_{m1} & a_{m2} & \cdots & a_{mn} & b_m\\
			\end{array}\right]
		\end{displaymath}
		
\end{document}

%Introduction to Linear Algebra 4th ed, Gilbert Strang