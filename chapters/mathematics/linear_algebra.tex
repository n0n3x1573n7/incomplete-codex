\documentclass{report}

\begin{document}
The target of Linear Algebra is to solve a system of homogenous linear equations. To do so, we deal with vectors and matrices.
	\section{Vector Spaces}
		For the definitions on vector spaces, subspaces, and bases, refer to the chapter \ref{chap_vector_space}.
		\subsection{Linear Independence}
		We now define linear independence, one of the most important concepts utilized in linear algebra.
		\begin{defn}[Linear Independence]
			if $S=\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_r}\}$ is a nonempty set of vectors in a vector space $V$, then the vector equation $k_1\vec{v_1}+k_2\vec{v_2}+\dots+k_r\vec{v_r}=\vec{0}$ has at least one solution, namely, $k_1=0, k_2=0, \dots, k_r=0$, the \emph{trivial solution}. If this is the only solution, then $S$ is said to be a \emph{linearly independent set}. If there are solutions in addition to the trivial solution, then $S$ is said to be \emph{linearly dependent}.
		\end{defn}
		
		\begin{thm}
			Let $S=\{\vec{v_1}, \vec{v_2}, \dots, \vec{v_r}\}$ be a set of vectors in $\mathbb{R}^n$. If $r>n$, then $S$ is linearly dependent.
		\end{thm}
		
		\subsection{Orthogonality}
		\begin{defn}[Euclidean Inner Product]
			Let $\vec{u}=(u_1,u_2,\dots,u_n)$ and $\vec{v}=(v_1,v_2,\dots,v_n)$ in $\mathbb{R}^n$. The inner product of the two vectors $\vec{u}$ and $\vec{v}$ is defined as
			\begin{displaymath}
				\vec{u}\cdot\vec{v}=\sum_{i=1}^{n}u_iv_i=u_1v_1+u_2v_2+\dots+u_nv_n
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Norm]
			The \emph{norm} of $\vec{u}=(u_1,u_2,\dots,u_n)$ in $\mathbb{R}^n$, denoted $\|\vec{u}\|$, is defined by
			\begin{displaymath}
			\|\vec{u}\|=\sqrt{\vec{u} \cdot \vec{u}}=\sqrt{\sum_{i=1}^{n}u_i^2}=\sqrt{u_1^2+u_2^2+\dots+u_n^2}
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Distance]
			If $\vec{u}=(u_1,u_2,\dots,u_n)$ and $\vec{v}=(v_1,v_2,\dots,v_n)$ are vectors in $\mathbb{R}^n$, then the \emph{distance} between $\vec{u}$ and $\vec{v}$, denoted $d(\vec{u},\vec{v})$, and define it to be:
			\begin{displaymath}
				d(\vec{u},\vec{v})=\|\vec{u}-\vec{v}\|=\sqrt{(u_1-v_1)^2+(u_2-v_2)^2+\cdots+(u_n-v_n)^2}
			\end{displaymath}
		\end{defn}
		
		\begin{defn}[Unit vectors]
			A vector $\vec{u}$ in $\mathbb{R}^n$ is said to be a unit vector iff $\|\vec{u}\|=1$.
		\end{defn}
		
		\begin{defn}[Angle]
			The \emph{angle} between two nonzero vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ is defined by
			\begin{displaymath}
				\theta=\cos^{-1}(\frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|\|\vec{v}\|})
			\end{displaymath}
		\end{defn}
		
		\begin{thm}[Cauchy-Schwarz Inequality]
			If $\vec{u}=(u_1,u_2,\dots,u_n)$ and $\vec{v}=(v_1,v_2,\dots,v_n)$ are vectors in $\mathbb{R}^n$, then $\left|\vec{u}\cdot\vec{v}\right| \le \|u\|\|v\|$. In terms of components:
			\begin{displaymath}
				\left|u_1v_1+u_2v_2+\cdots+u_nv_n\right| \le \left(u_1^2+u_2^2+\cdots+u_n^2\right)^{1/2} \left(v_1^2+v_2^2+\cdots+v_n^2\right)^{1/2}
			\end{displaymath}
		\end{thm}
		
		\begin{thm}[Triangle Inequality]
			If $\vec{u}, \vec{v}, \vec{w}$ are vectors in $\mathbb{R}^n$, then:
			\begin{itemize}
				\item $\|\vec{u}+\vec{v}\| \ge \|\vec{u}\|+\|\vec{v}\|$: Triangle Inequality for Vectors
				\item $d(\vec{u},\vec{v}) \ge d(\vec{u},\vec{v})+d(\vec{w},\vec{v})$: Triangle Inequality for Distances
			\end{itemize}
		\end{thm}
		
		\begin{thm}[Equations for Vectors within the Euclidean Space]
			If $\vec{u}$ and $\vec{v}$ are vectors in $\mathbb{R}^n$
			\begin{itemize}
				\item $\|\vec{u}+\vec{v}\|^2+\|\vec{u}-\vec{v}\|^2=2\left(\|u\|^2+\|v\|^2\right)$: Parallelogram Equation for Vectors
				\item $\vec{u}\cdot\vec{v}=\frac{1}{4}\|\vec{u}+\vec{v}\|^2-\frac{1}{4}\|\vec{u}-\vec{v}\|^2$
			\end{itemize}
		\end{thm}
		
		\begin{defn}[Orthogonal Vectors]
			Two nonzero vectors $\vec{u}$ and $\vec{v}$ in $\mathbb{R}^n$ are said to be \emph{orthogonal} or \emph{perpendicular} if $\vec{u} \cdot \vec{v} = 0$.
		\end{defn}
		
		\begin{defn}[Orthogonal set]
			A nonempty set of vectors in $\mathbb{R}^n$ is called an \emph{orthogonal set} if all pairs of distinct vectors in the set are orthogonal. If they are also all unit vectors, it is called an \emph{orthonormal set}.\\
			In other words, for a set $\{u_1, u_2, \dots, u_n\}$ to be orthogonal:
			\begin{displaymath}
				u_i \cdot u_j
				\begin{cases}
					\|u_i\|^2 & i=j\\
					0 & i \ne j
				\end{cases}
			\end{displaymath}
			And for the set to be orthonormal, in addition to above, $\forall i \in \{1, 2, \dots n\}, \|u_i\|=1$.
		\end{defn}
		
		\begin{defn}[Orthogonal Complement]
			The \emph{orthogonal complement} of a subspace $W$ of a vector space $V$, denoted $W^\perp$, is defined to be the set of all vectors in $V$ that are orthogonal to every vector of $W$.
		\end{defn}
		
		\begin{thm}
			Suppose $W$ is a subspace of a vector space $V$. $W^\perp$ is also a subspace of $V$.
		\end{thm}
		
		\begin{thm}[Projection Theorem]
			If $\vec{u}$ and $\vec{a}$ are vectors in $\mathbb{R}^n$ and if $\vec{a} \ne \vec{0}$, then $\vec{u}$ can be expresses in exactly one way in the form $\vec{u}=\vec{w_1}+\vec{w_2}$, where $\exists k \in \mathbb{R}, \vec{w_1}=k\vec{a}$ and $\vec{a} \cdot \vec{w_2}=0$.
		\end{thm}
		
		In the theorem above, the vector $\vec{w_1}$ is called the \emph{orthogonal projection of $\vec{u}$ on $\vec{a}$} or \emph{vector component of $\vec{u}$ along $\vec{a}$}, and the vector $\vec{w_2}$ is called the \emph{vector component of $\vec{u}$ orthogonal to $\vec{a}$}.
		
		\begin{defn}[Projection]
			Let $\vec{u}$ and $\vec{a} \ne \vec{0}$ be vectors in $\mathbb{R}^n$. The \emph{orthogonal projection of $\vec{u}$ on $\vec{a}$}, denoted $\verb|proj|_{\vec{a}}\vec{u}$
		\end{defn}
	
	\section{Matrix}
		\subsection{Matrices and its operations}
			\begin{defn}[Matrix]
				A \emph{matrix} is a rectangular array of numbers. The numbers in the array are called the \emph{entries} in the matrix.
			\end{defn}
		
			Equality, addition, and subtraction can only be defined on same-sized matrices, and is defined elementwise; scalar multiplication is also defined elementwise.
			
			\begin{defn}[Matrix Multiplication]
				If $A$ is an $m \times r$ matrix and $B$ is an $r \times n$ matrix, then the \emph{product} $AB$ is the $m \times n$ matrix whose entries are determined as follows:
				The entry of $AB$ on row $i$ and column $j$, multiply the corresponding entries from the row $i$ from $A$ and column $j$ from $B$, then add them all together.
			\end{defn}
		
			Matrices of the same size may be used in a linear combination, just like vectors[\ref{def_linear_combination_vector}].
			
			\begin{defn}[Linear Combination of a Matrix]
				If $A_1, A_2, \dots, A_r$ are matrices of the same size, and if $c_1, c_2, \dots, c_r$ are scalars, then an expression of the form
				\begin{displaymath}
					c_1A_1+c_2A_2+\cdots+c_rA_r
				\end{displaymath}
				is called a \emph{linear combination} of $A_1, A_2, \dots, A_r$ with coefficients $c_1, c_2, \dots, c_r$.
			\end{defn}
			
			\begin{thm}
				If $A$ is an $m \times n$ matrix and if $\vec{x}$ is an $n \times 1$ column vector, then the product $A\vec{x}$ can be expressed as a linear combination of the column vectors of $A$ in which the coefficients are the entries of $\vec{x}$.
			\end{thm}
			
			\begin{defn}[Transpose]
				For any $m \times n$ matrix, then the \emph{transpose} of $A$, denoted by $A^T$, is defined to be the $n \times m$ matrix that results by interchanging the rows and columns of $A$; that is, the first column of $A^T$ is the first row of $A$ and so forth.
			\end{defn}
			
			\begin{defn}[Trace]
				For a square matrix $A$, the \emph{trace} of $A$, denoted $tr(A)$, is defined to be the sum of the entries on the main diagonal of $A$.
			
		\end{defn}
	
	\section{Inverse}
		\subsection{Elementary Row Operations and Matrices}
		
		\begin{defn}[Elementary Row Operations]\label{def_elementary_row_operations}
			The following three operations are said to be the \emph{elementary row operations} on a matrix:
			\begin{enumerate}
				\item Multiply a row through by a nonzero constant.
				\item Interchange two rows.
				\item Add a constant times one row to another.
			\end{enumerate}
		\end{defn}
		
		\begin{defn}[Elementary Row Matrices]
			An $n \times n$ matrix is called an \emph{elementary matrix} if it can be obtained from the $n \times n$ identity matrix $I_n$ by performing a single elementary row operation.
		\end{defn}
		
		\begin{thm}[Elementary Row Operations and Elementary Row Matrices]
			If the elementary matrix $E$ results from performing a certain row operation on $I_m$ and $A$ is an $m \times n$ matrix, then the product $EA$ is the matrix that results when this same row operation is performed on $A$.
		\end{thm}
		
		\begin{defn}[Reduced-row Echelon Form]
			A matrix that is in its \emph{reduced-row echelon form(rref)} has the following properties:
			\begin{enumerate}
				\item If a row does not consist entirely of zeroes, then the first nonzero number in the row is a 1. We call this a \emph{leading 1}.
				\item If there are any rows that consist entirely of zeroes, then they are grouped together at the bottom of the matrix.
				\item In any two successive rows that do not consist entirely of zeroes, the leading 1 in the lower row occurs farther to the right than the leading 1 in the higher row.
				\item Each column that contains a leading 1 has zeroes everywhere else in that column
			\end{enumerate}
			A matrix that has the first three properties is said to be in \emph{row echelon form}.
		\end{defn}
		
		\begin{thm}
			If $R$ is the reduced row echelon form of an $n \times n$ matrix $A$, then either $R$ has a row of zeroes or $R$ is the identity matrix $I_n$.
		\end{thm}
		
		There are two important facts on echelon forms:
		\begin{enumerate}
			\item Every matrix has a unique rref.
			\item Row echelon forms are not unique, but, they have the same:
			\begin{itemize}
				\item number of zero rows
				\item positions of leading 1's
					\subitem the positions are called the \emph{pivot positions} of A
					\subitem the columns are called the \emph{pivot column} of A
			\end{itemize}
		\end{enumerate}
		
		\begin{mthd}[Gauss-Jordan Elimination]\label{mthd_gauss_jordan_elim}
			This method will use elementary row operations and through two phases, forward and backward phases, reduces a matrix into its reduced row echelon form.
			\begin{enumerate}[label=Phase \arabic*.]
				\item Forward Phase\footnote{If only this phase is used to produce a row echelon form, this is called the Gaussian elimination.}
				\begin{enumerate}[label=Step \arabic*.]
					\item Locate the leftmost column that does not consist entirely of zeroes.
					\item Interchange the top row with another row, if necessary, to bring a nonzero entry to the top of the column found in Step 1.
					\item Multiply the first row by a constant so that it has a leading 1.
					\item Add suitable multiples of the top row to the rows below so that all entries below the leading 1 become zeroes.
					\item Reapply Step 1, ignoring the upper rows until the entire matrix is in row echelon form.
				\end{enumerate}
				\item Backward Phase
				\begin{enumerate}[label=Step \arabic*.]
					\setcounter{enumii}{6}
					\item Beginning with the last nonzero row and working upward, add suitable multiples of each row to the rows above to make the entries above the leading 1's to 0.
				\end{enumerate}
			\end{enumerate}
		\end{mthd}
		
		\subsection{Finding the Inverse for a Matrix}
		\begin{defn}[Inverse]
			If $A$ is a square matrix, and if a matrix $B$ of the same size can be found so that $AB=BA=I$, then $A$ is said to be \emph{invertible} or \emph{nonsingular} and $B$ is called an \emph{inverse} of $A$, denoted by $A^{-1}$. If no such matrix $B$ can be found, then $A$ is said to be \emph{singular} or \emph{non-invertible}.
		\end{defn}
		
		\begin{thm}
			If $B$ and $C$ are both inverses of the matrix $A$, then $B=C$.
		\end{thm}
		
		\begin{thm}[Inverse of a 2-by-2 matrix]\label{thm_inv_2_by_2}
			The matrix
			\begin{displaymath}
				A=
				\begin{bmatrix}
					a & b \\ c & d
				\end{bmatrix}
			\end{displaymath}
			is invertible iff $ad-bc\ne0$, in which case the inverse is given by:
			\begin{displaymath}
				A^{-1}=\frac{1}{ad-bc}
				\begin{bmatrix}
					d & -b \\ -c & a
				\end{bmatrix}
			\end{displaymath}
		\end{thm}
		
		\begin{thm}
			If $A$ and $B$ are invertible matrices with the same size, then $AB$ is invertible and $(AB)^{-1}=B^{-1}A^{-1}$.\\
			In general, a product of any number of invertible matrices is invertible, and the inverse of the product is the product of the inverses in the reverse order.
		\end{thm}
		
		\begin{thm}
			If $A$ is invertible, then $A^T$ is also invertible, and $(A^T)^{-1}=(A^{-1})^T$.
		\end{thm}
		
		\begin{thm}
			Every elementary matrix is invertible, and the inverse is also an elementary matrix.
		\end{thm}
		
		\begin{mthd}[Inversion Algorithm]
			To find the inverse of an invertible matrix $A$, find a sequence of elementary row operations that reduces $A$ to the identity and then perform that same sequence of operations on $I_n$ to obtain $A^{-1}$.
			
			For easier approach, simply use Gauss-Jordan Elimination[\ref{mthd_gauss_jordan_elim}] to the augmented matrix $\left[A|I_n\right]$ so that it becomes $\left[I_n|A^{-1}\right]$.
		\end{mthd}
	
	\section{Determinants}
		Recall from [\ref{thm_inv_2_by_2}] that the $2 \times 2$ matrix $A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ is invertible iff $ad-bc \ne 0$. The term $ad-bc$ is the determinant of the matrix $A$. \emph{Determinant} is a scalar value that can be computed from the elements of a square matrix which encodes certain properties of the matrix.
		\subsection{Calculating Determinants}
			There are two methods to calculate the determinant.
			\subsubsection{Method of Cofactor Expansion}
				\begin{defn}[Minors and Cofactors]
					Let $A$ be a square matrix. Then the \emph{minor of entry $a_{ij}$}, denoted by $M_ij$, is defined to be the determinant of the submatrix that remains after the i-th row and j-th column are deleted from $A$. The number $C_{ij}=(-1)^{i+j}M_{ij}$ is called the \emph{cofactor of entry $a_{ij}$}.
				\end{defn}
				
				\begin{defn}[Adjoint]
					If $A$ is $n \times n$ matrix and $C_{ij}$ is the cofactor of $a_{ij}$, then the matrix
					\begin{displaymath}
					\begin{bmatrix}
						C_{11} & C_{12} & \cdots & C_{1n} \\
						C_{21} & C_{22} & \cdots & C_{2n} \\
						\vdots & \vdots &        & \vdots \\
						C_{n1} & C_{n2} & \cdots & C_{nn} \\
					\end{bmatrix}
					\end{displaymath}
					is called the \emph{matrix of cofactors from $A$}. The transpose of this matrix is called the \emph{adjoint of $A$}, denoted by adj$(A)$.
				\end{defn}
				
				\begin{defn}[Determinant]
					If $A$ is an $n \times n$ matrix, then the number obtained by multiplying the entries in any row or column of $A$ by the corresponding cofactors and adding the resulting products is called the \emph{determinant} of $A$, and the sums themselves are called \emph{cofactor expansions} of $A$.
					
					The cofactor expansion along the j-th column is as follows:
					\begin{displaymath}
						\det(A)=a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}
					\end{displaymath}
					
					and the cofactor expansion along the i-th row is as follows:
					\begin{displaymath}
						\det(A)=a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in}
					\end{displaymath}
				\end{defn}
				
				\begin{thm}\label{thm_triangular_determinant}
					If $A$ is an $n \times n$ triangular matrix, then $\det(A)$ is the product of entries on the main diagonal of the matrix; that is, $\det(A)=a_{11}a_{22}\cdots a_{nn}$.
				\end{thm}
			
			\subsubsection{Method of Elementary Row Operations}
				This section presents a series of theorems that can be proven with the cofactor expansion formula that will suffice by themselves, paired with the theorem for determinants for triangular matrices[\ref{thm_triangular_determinant}](or simply the fact that $\forall n, \det(I_n)=1$), to find the determinant by continuously applying elementary row operations to the target matrix.
				\begin{thm}
					Let $A$ be a square matrix. If $A$ has a row of zeroes or a column of zeroes, then $\det(A)=0$.
				\end{thm}
				
				\begin{thm}
					Let $A$ be a square matrix. Then $\det(A)=\det(A^T)$.
				\end{thm}
				
				\begin{thm}\label{thm_row_ops_det}
					Let $A$ be an $n \times n$ matrix.
					\begin{enumerate}
						\item If $B$ is the matrix that results when a single row or single column or $A$ is multiplies by a scalar $k$, then $\det(B)=k\det(A)$.
						\item If $B$ is the matrix that results when two rows or two columns of $A$ are interchanged, then $\det(B)=-\det(A)$.
						\item If $B$ is the matrix that results when a multiple of one row of $A$ is added to another row or when a multiple of one column is added another column, then $\det(B)=\det(A)$.
					\end{enumerate}
				\end{thm}
				
				\begin{coro}
					Let $E$ be an $n \times n$ matrix.
					\begin{enumerate}
						\item If $E$ results from multiplying a row of $I_n$ by a nonzero number $k$, then $\det(E)=k$.
						\item If $E$ results from interchanging two rows of $I_n$, then $det(E)=-1$.
						\item If $E$ results from adding a multiple of one row of $I_n$ to another, then $\det(E)=1$.
					\end{enumerate}
				\end{coro}
				
				\begin{thm}
					If $A$ is a square matrix with two proportional rows or two proportional columns, then $\det(A)=0$.
				\end{thm}
				
				Often times, the method of elementary row operations may be applied partially to assist with cofactor expansion formula.
			
		\subsection{Properties of Determinants}
			\begin{thm}
				Let $A$, $B$, and $C$ be $n \times n$ matrices that differ only in a single row, say the r-th row, and assume that the r-th row of $C$ can be obtained by adding corresponding entries in the r-th row of $A$ and $B$. Then, $\det(C)=\det(A)+\det(B)$.
				
				The same result holds for columns.
			\end{thm}
			
			Pairing the theorem above with theorem \ref{thm_row_ops_det}'s first fact, we can say that the determinant is a linear function of each row separately.
			
			\begin{thm}
				A square matrix $A$ is invertible iff $\det(A) \ne 0$.
			\end{thm}
			
			\begin{thm}
				If $A$ and $B$ are square matrices of the same size, then $\det(AB)=\det(A)\det(B)$.
			\end{thm}
			
			\begin{thm}
				If $A$ is invertible, then
				\begin{displaymath}
					\det(A^{-1})=\frac{1}{\det(A)}
				\end{displaymath}
			\end{thm}
			
			\begin{thm}[Inverse of a Matrix using its Adjoint]
				If $A$ is an invertible matrix, then
				\begin{displaymath}
					A^{-1}=\frac{1}{\det(A)}\verb|adj|(A)
				\end{displaymath}
			\end{thm}
			
			\begin{thm}[Cramer's Rule]
				If $A\bf{x}=\bf{b}$ is a system of n linear equations such that $det(A)\ne0$, then the system has a unique solution. The solution is:
				\begin{displaymath}
					\forall i, x_i=\frac{\det(A_i)}{\det(A)}
				\end{displaymath}
				where $A_i$ is the matrix obtained by replacing the entries in the j-th column of $A$ by the entries in the matrix $\bf{b}$.
			\end{thm}
		
	\section{Matrices and Vector Spaces}
		\subsection{Fundamental Spaces of a Matrix}
		There are four important vector spaces on any given matrix, which are row, column, null, and left null spaces.
		\begin{defn}[Fundamental Spaces of a Matrix]
			\begin{itemize}
				\item A \emph{Column space} of a matrix, denoted $im(A)$(image of $A$), $range(A)$(range of $A$), $col(A)$ or $C(A)$, is the vectors spanned by the column vectors of the matrix.
				\item A \emph{Row space} of a matrix, denoted $col(A^T)$, and sometimes called the coimage, is the vectors spanned by the row vectors of the matrix.
				\item A \emph{Null space} of a matrix, denoted $ker(A)$(kernel of $A$), $null(A)$ or $N(A)$, is the vector space of the solution vectors of the equation $A\bf{x}=\bf{0}$.
				\item A \emph{Left Null space} of a matrix, denoted $null(A^T)$, and sometimes called the cokernel, is the vector space of the solutions vectors of the equation $A^T\bf{y}=\bf{0}$.
			\end{itemize}
			The four spaces together are called the fundamental spaces of a matrix.
		\end{defn}
		
		\begin{defn}[Rank of a Matrix]
			The rank of a matrix $A$, denoted $rank(A)$, is defined to be the dimension of the column space.
		\end{defn}
		
		\begin{defn}[Full Rank]
			A matrix is said to have \emph{full rank} if its rank is largest possible among the matrices of the same dimensions, which is the minimum of the number of rows and columns.
		\end{defn}
		
		\begin{thm}
			$rank(A)$ equals the number of nonzero rows in $rref(A)$.
		\end{thm}
		
		From the definitions above, we gain the fundamental theorem of linear algebra.
		
		\begin{thm}[Fundamental Theorem of Linear Algebra, Pt. 1]
			Suppose a matrix A is $m \times n$. Let $r=rank(A)$. Fundamental subspaces of the matrix $A$ has the following dimensions:

			\begin{tabular}{ccc}
				\textbf{Name of Subspace} & \textbf{Containing Space} & \textbf{Dimension} \\
				Column Space ($C(A)$)             & $\mathbb{R}^m$            & $rank(A)=r$        \\
				Null Space ($N(A)$)               & $\mathbb{R}^n$            & $nullity(A)=n-r$   \\
				Row Space ($C(A^T)$)              & $\mathbb{R}^n$            & $rank(A)=r$        \\
				Left Nullspace ($N(A^T)$)         & $\mathbb{R}^m$            & $corank(A)=m-r$   
			\end{tabular}
		\end{thm}
		
		\begin{thm}[Fundamental Theorem of Linear Algebra, Pt. 2]
			\begin{itemize}
				\item $N(A)^\perp=C(A^T)$ in $\mathbb{R}^n$, that is, nullspace and row space are orthogonal complements.
				\item $C(A)^\perp=N(A^T)$ in $\mathbb{R}^m$, that is, column space and left null space are orthogonal complements.
			\end{itemize}
		\end{thm}
		
		\subsection{Matrix Transformation}
			\subsubsection{Translating General Vector Spaces into $\mathbb{R}^n$}
			
			\subsubsection{Matrix Transformations from $\mathbb{R}^n$ to $\mathbb{R}^m$}
		
	\section{Eigenvalues and Eigenvectors}
		\subsection{Characteristic Polynomial}
			\begin{defn}[Characteristic Polynomial]\label{def_characteristic_polynomial}
				The \emph{characteristic polynomial} of an $n \times n$ square matrix $A$ is defined to be
				\begin{displaymath}
					p_A(t)=\det(tI-A)
				\end{displaymath}
			\end{defn}
			To name some of the properties of any given characteristic polynomial themselves, they are monic(meaning that the leading coefficient is 1), and has degree $n$.
			
			The characteristic polynomial encodes many properties of the matrix. The most obvious property would be $p_A(t)=\det(A)$, and a slightly less obvious one would be that the coefficient of the term $t^{n-1}$ equals tr$(A)$.
			
			You might be wondering, "Hey, why is this guy talking about some kind of polynomial in a chapter about eigenvalues and eigenvectors?" Well, that'll become obvious once you see the definition of eigenvalues and eigenvectors.
		
		\subsection{Eigenvalues and Eigenvectors}
		\begin{defn}[Eigenvalues and Eigenvectors]
			If $A$ is an $n \times n$ matrix, then a nonzero vector $\bf{x}$ in $\mathbb{R}^n$ is called an \emph{eigenvector} of $A$ if $A\bf{x}=\lambda \bf{x}$ for some $\lambda \in \mathbb{R}$. $\lambda$ is called an \emph{eigenvalue} of $A$, and $\bf{x}$ is said to be an \emph{eigenvector corresponding to $\lambda$}.
		\end{defn}
		
		Now since $A\bf{x}=\lambda \bf{x}=\lambda I_n \bf{x}$, it follows that $(\lambda I_n - A)\bf{x}=\bf{0}$. Hey, haven't we seen that equation before for the characteristic polynomial[\ref{def_characteristic_polynomial}]? We can change this equation into $p_A(\lambda)=0$, therefore the following theorem:
		
		\begin{thm}
			For an $n \times n$ matrix A, $\lambda$ is an eigenvalue of $A$ iff $p_A(\lambda)=\det(\lambda I-A)=0$.
		\end{thm}
		
		Now, visiting the
		\ifcsname r@thm_fundamental_theorem_of_algebra \endcsname
		Fundamental Theorem of Algebra[\ref{thm_fundamental_theorem_of_algebra}]%
		\else
		\href{https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{Fundamental Theorem of Algebra}
		\fi
		, we can see that there are exactly $n$ (possibly complex and multiple) roots of a characteristic polynomial of $n \times n$ matrix, and therefore can have up to $n$ distinct eigenvalues.
	
	\section{Special Matrices}
		\subsection{Diagonal Matrices}
		A \emph{diagonal matrix} is a square matrix in which all entries off the main diagonal are zero. They can be represented in the following form:
		
		\begin{displaymath}
		D=
		\begin{bmatrix}
		d_1 & 0 & \cdots & 0 \\
		0 & d_2 & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n \\
		\end{bmatrix}
		\end{displaymath}
		
		A diagonal matrix is invertible iff all of its diagonal entries are nonzero, and its inverse is:
		
		\begin{displaymath}
		D^{-1}=
		\begin{bmatrix}
		d_1^{-1} & 0 & \cdots & 0 \\
		0 & d_2^{-1} & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n^{-1} \\
		\end{bmatrix}
		\end{displaymath}
		
		It is easy to calculate powers of diagonal matrices. More specifically,
		
		\begin{displaymath}
		D^{k}=
		\begin{bmatrix}
		d_1^{k} & 0 & \cdots & 0 \\
		0 & d_2^{k} & \cdots & 0 \\
		\vdots & \vdots &        & \vdots \\
		0 & 0 & \cdots & d_n^{k} \\
		\end{bmatrix}
		\end{displaymath}
		
		\subsection{Triangular Matrices}
		A \emph{lower triangular} matrix is a matrix in which all the entries above the main diagonal are zero; an \emph{upper triangular} matrix is a matrix in which all the entries below the main diagonal are zero. Either of they are called \emph{triangular}. They can be represented in the following form:
		
		\begin{displaymath}
		L=
			\begin{bmatrix}
				l_{11}     & 0          & \cdots & 0              & 0      \\
				l_{21}     & l_{22}     & \cdots & 0              & 0      \\
				\vdots     & \vdots     &        & \vdots         & \vdots \\
				l_{(n-1)1} & l_{(n-1)2} & \cdots & l_{(n-1)(n-1)} & 0      \\
				l_{n1}     & l_{n2}     & \cdots & l_{n(n-1)}     & l_{nn} \\
			\end{bmatrix}
		, U=
			\begin{bmatrix}
				u_{11} & u_{12} & \cdots & u_{1(n-1)}     & u_{1n}     \\
				0      & u_{22} & \cdots & u_{2(n-1)}     & u_{2n}     \\
				\vdots & \vdots &        & \vdots         & \vdots     \\
				0      & 0      & \cdots & u_{(n-1)(n-1)} & u_{(n-1)n} \\
				0      & 0      & \cdots & 0              & u_{nn}     \\
			\end{bmatrix}
		\end{displaymath}
		
		\begin{thm}
			\begin{enumerate}
				\item The transpose of a lower triangular matrix is upper triangular, and vice versa.
				\item The product of lower triangular matrices is lower triangular, and same for the upper triangular matrices.
				\item A triangular matrix is invertible iff its diagonal entries are all nonzero.
				\item The inverse of an invertible lower triangular matrix is lower triangular, and same for the invertible upper triangular matrices.
			\end{enumerate}
		\end{thm}
		
		\subsection{Symmetric Matrices}
		A \emph{symmetric} matrix is a square matrix such that $S=S^T$. Specifically, $S$ is symmetric iff $\forall 1 \le i,j \le n, S_{ij}=S_{ji}$.
		
		It is important to note that for two symmetric matrices $A$ and $B$, $(AB)^T=B^TA^T=BA$, and therefore their product is not guaranteed to be symmetric unless $AB=BA$, that is, $A$ and $B$ commute.
		
		\begin{thm}
			The product of two symmetric matrices is symmetric iff the matrices commute.
		\end{thm}
		
		In general, a symmetric matrix may not be invertible. However if they are, the following theorem shows an interesting fact:
		
		\begin{thm}
			If $A$ is an invertible symmetric matrix, then $A^{-1}$ is symmetric.
		\end{thm}
	
	\section{Solving Linear Equations}
	We come to this final section, the ultimate target of linear algebra: solving a system of linear equations.
		\subsection{Linear Equations to Matrices}
		A finite set of linear equations is called a \emph{system of linear equations}, or more briefly, a \emph{linear system}. The variables are called \emph{unknowns}.
		
		\begin{center}
			\begin{tabular}{ccccccccc}
				$a_{11}x_1$ & $+$ & $a_{12}x_2$ & $+$ & $\cdots$ & $+$ & $a_{1n}x_n$ & $=$ & $b_1$    \\
				$a_{21}x_1$ & $+$ & $a_{22}x_2$ & $+$ & $\cdots$ & $+$ & $a_{2n}x_n$ & $=$ & $b_2$    \\
				$\vdots$    &     & $\vdots$   &     &          &     & $\vdots$    &     & $\vdots$ \\
				$a_{m1}x_1$ & $+$ & $a_{m2}x_2$ & $+$ & $\cdots$ & $+$ & $a_{mn}x_n$ & $=$ & $b_m$   
			\end{tabular}
		\end{center}
		
		A \emph{solution} of a linear system in $x_1,x_2,\dots,x_n$ is a sequence of $n$ numbers $s_1,s_2,\dots,s_n$ for which the substitution $x_i=s_i$ makes each equation a true statement.
		
		We say that a linear system is \emph{consistent} if it has at least one solution and \emph{inconsistent} if it has no solutions.
		
		\begin{thm}
			A system of linear equations has zero, one, or infinitely many solutions. There are no possibilities.
		\end{thm}
		
		If a linear system has infinitely many solutions, then a set of parametric equations from which all solutions can be obtained by assigning numerical values to the parameters is called a \emph{general solution} of the system.
		
		If all constant terms are zero, that is, $\forall i, b_i=0$, it is said to be \emph{homogeneous}. A homogeneous system of linear equations always is consistent since it has $\forall i, x_i=0$ as its solution: this is called the \emph{trivial solution}. If there are other solutions, they are called the \emph{nontrivial solution}.
		
		The system of linear equations above can be represented in a matrix multiplication as shown below:
		
		\begin{displaymath}
			\begin{bmatrix}
				a_{11} & a_{12} & \cdots & a_{1n} \\
				a_{21} & a_{22} & \cdots & a_{2n} \\
				\vdots & \vdots &        & \vdots \\
				a_{m1} & a_{m2} & \cdots & a_{mn} \\
			\end{bmatrix}
			\begin{bmatrix}
				x_1 \\ x_2 \\ \vdots \\ x_n
			\end{bmatrix}
			=
			\begin{bmatrix}
				b_1 \\ b_2 \\ \vdots \\ b_m
			\end{bmatrix}
		\end{displaymath}
		
		By designating the three matrices $A$, $\bf{x}$ and $\bf{b}$ respectively, we can say that $A\bf{x}=\bf{b}$. In this equation, $A$ is called the \emph{coefficient matrix} of the system.
		
		The \emph{augmented matrix} for the system is obtained by adjoining $\bf{b}$ to $A$ as the last column as follows:
		
		\begin{displaymath}
			\left[\begin{array}{cccc|c}
				a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
				a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
				\vdots & \vdots &        & \vdots & \vdots\\
				a_{m1} & a_{m2} & \cdots & a_{mn} & b_m\\
			\end{array}\right]
		\end{displaymath}
		
		Note the correspondence between basic algebraic operations on a given set of linear systems and elementary row operations on the augmented matrix of the said systems. In the order in the definition [\ref{def_elementary_row_operations}], the correspondences are:
		
		\begin{enumerate}
			\item Multiply an equation through by a nonzero constant
			\item Interchange two equations	
			\item Add a constant times one equation to another.
		\end{enumerate}
		
		By applying elementary row operations to the augmented matrix, we can get to the point where the augmented matrix is reduced to its reduced row echelon form. The variables corresponding to the leading 1's in the augmented matrix is called the \emph{leading variables}. The remaining variables are called \emph{free variables}.
		
		There is an important theorem regarding the number of free variables and homogeneous systems:
		
		\begin{thm}[Free Variable Theorem for Homogeneous Systems]
			If a homogeneous linear system has $n$ unknowns, and if the rref of its augmented matrix has $r$ nonzero rows, then the system has $n-r$ free variables.
		\end{thm}
		
		\begin{coro}
			A homogeneous linear system with more unknowns than equations has infinitely many solutions.
		\end{coro}
		
		In the following sections on finding solutions or parametric equation for solutions where it applies, the coefficient matrix will be noted as $A$, the vector of variables will be noted as $\bf{x}$ and the variables as $x_1, x_2, \dots, x_n$, and the vector for the constants as $\bf{b}$.
		
		\subsection{Method of Inverses}
		
		This can be used iff $A$ is an invertible matrix.
		
		Find the inverse of $A$, $A^{-1}$.
		The only possible solution is $\bf{x}=A^{-1}\bf{b}$.
		
		\subsection{Method of RREF}\label{mthd_rref}
		
		This can be used for any matrix $A$.
		
		\begin{enumerate}
			\item Reduce the augmented matrix $\left[A|\bf{b}\right]$ to its RREF $\left[R|\bf{c}\right]$
			\item See if $R$ has a zero row. If any of the value of $\bf{c}$ corresponding to the zero row is nonzero, the system is inconsistent.
			\item Exchange the free variables with parametric variables.
			\item Transpose the free variables to RHS so the leading variables(the pivots) are the only ones left on the LHS.
			\item The resulting expressions are the parametric equation for solutions.
		\end{enumerate}
		
		\subsection{Method of Particular and Special Special Solutions}
			
			This can be used for any matrix $A$.
			
			This method is extremely similar to Method of RREF[\ref{mthd_rref}].
			
			In this method, we first find the nullspace of $A$.
			
			\begin{thm}
				If $A$ is an $m \times n$ matrix, then the solution set of the homogeneous linear system $A\bf{x}=\bf{0}$ consists of all vectors in $\mathbb{R}^n$ that are orthogonal to every row vector of $A$.
			\end{thm}
			
			\begin{thm}
				The general solution of a consistent linear system $A\bf{x}=\bf{b}$ can be obtained by adding any specific solution of $A\bf{x}=\bf{b}$ to the general solution of $A\bf{x}=\bf{0}$.
			\end{thm}
			
			The theorem above indicates that we need to find the nullspace of $A$ along with a specific solution of $A\bf{x}=\bf{b}$ to find the whole, general solution of $A\bf{x}=\bf{b}$.
			
			Using Gauss-Jordan Elimination[\ref{mthd_gauss_jordan_elim}], we reduce the augmented matrix $\left[A|\bf{b}\right]$ to its rref, and detect if there are any inconsistencies. This corresponds to the first step on Method of RREF.
			
			First, we find the nullspace for $A$. We get the basis vectors from the rref of $A$. This corresponds to the second and third steps on Method of RREF.
			
			Next, we find the specific solution of $A\bf{x}=\bf{b}$. From the rref of $\left[A|\bf{b}\right]$ say $\left[R|\bf{c}\right]$, solve the equation by setting all free variables to 0. In doing so, since all leading variables have coefficient 1, the values of $c$ immediately correspond to the specific solution of the leading variables. This process is therefore almost automatic.
			
			Now we have the nullspace of $A$ and the specific solution of $A\bf{x}=\bf{b}$; add those two together to gain the whole solution. This corresponds to the fourth and final steps on Method of RREF.
		
	
	Before ending this chapter, we summarize this chapter by gathering all the facts on invertible matrices, written in the appendix[\ref{equiv_invert}].
\end{document}

%Introduction to Linear Algebra 4th ed, Gilbert Strang